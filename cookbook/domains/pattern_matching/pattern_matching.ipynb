{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching\n",
    "\n",
    "This example shows how to evaluate a `genlm.control` model on the pattern matching domain.\n",
    "\n",
    "* **Task**: Generate strings that conform to expressive pattern-matching specifications. Compared to formal regular expressions, these patterns contain explicit features that cannot be fully captured by deterministic finite-state automata, including unbounded center embedding and conditionals. \n",
    "* **Data**: Over 400 pattern-matching specifications generated via the pipeline described in Appendix I of [(Lipkin et al., 2025)](https://arxiv.org/abs/2504.05410).\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the dependencies for this domain. In the root directory, run:    \n",
    "\n",
    "```bash\n",
    "pip install -e .[pattern_matching]\n",
    "```\n",
    "\n",
    "Second, download the `patterns.csv` file from the `assets/pattern_matching` directory in [the repository](https://github.com/genlm/genlm-eval/tree/main/assets/pattern_matching). (Note that you can also use your own patterns.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage \n",
    "\n",
    "This example shows how to evaluate a `genlm.control` model on the pattern matching domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the dataset and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from genlm.eval.domains.pattern_matching import (\n",
    "    PatternMatchingDataset,\n",
    "    PatternMatchingEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatternMatchingDataset.from_csv(\n",
    "    \"../../../assets/pattern_matching/patterns.csv\", pattern_column=\"regex\"\n",
    ")\n",
    "\n",
    "evaluator = PatternMatchingEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model adaptor\n",
    "\n",
    "A model adaptor is an async callable that takes a `PatternMatchingInstance` and returns a `ModelOutput`. For this example, we'll use a constrained `genlm.control.PromptedLLM` to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from genlm.control import PromptedLLM, AWRS\n",
    "from genlm.eval import ModelOutput, ModelResponse\n",
    "from genlm.eval.domains.pattern_matching import (\n",
    "    default_prompt_formatter,\n",
    "    PatternPotential,\n",
    ")\n",
    "\n",
    "# Load an LLM\n",
    "LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n",
    "\n",
    "\n",
    "async def model(instance, output_dir, replicate):\n",
    "    # Set the prompt for the LLM.\n",
    "    LLM.prompt_ids = default_prompt_formatter(\n",
    "        LLM.model.tokenizer, instance, use_chat_format=False\n",
    "    )\n",
    "\n",
    "    # Define a potential that ensures the generated text matches the pattern\n",
    "    potential = PatternPotential(instance.pattern).coerce(LLM, f=b\"\".join)\n",
    "\n",
    "    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n",
    "    sampler = AWRS(LLM, potential)\n",
    "\n",
    "    # Run SMC to sample sequences from the constrained model.\n",
    "    sequences = await sampler.smc(\n",
    "        n_particles=5,\n",
    "        ess_threshold=0.5,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    # Return the sampled sequences and their probabilities as a ModelOutput.\n",
    "    return ModelOutput(\n",
    "        responses=[\n",
    "            ModelResponse(response=sequence, weight=prob)\n",
    "            for sequence, prob in sequences.decoded_posterior.items()\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance instance_id=0 pattern='(?<!\\\\d{3})abc(?!\\\\d{3})'\n",
      "Mean weighted accuracy (instance): 1.0\n",
      "Mean weighted accuracy (total): 1.0\n",
      "\n",
      "Instance instance_id=1 pattern='^(?|(a)|(b)|(c))\\\\1$'\n",
      "Mean weighted accuracy (instance): 1.0\n",
      "Mean weighted accuracy (total): 1.0\n",
      "\n",
      "Instance instance_id=2 pattern='[\\\\p{IsAlphabetic}&&[\\\\P{L}]]'\n",
      "Mean weighted accuracy (instance): 1.0\n",
      "Mean weighted accuracy (total): 1.0\n",
      "\n",
      "Instance instance_id=3 pattern='^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\n",
      "Mean weighted accuracy (instance): 1.0\n",
      "Mean weighted accuracy (total): 1.0\n",
      "\n",
      "Instance instance_id=4 pattern='^[a-f0-9]{8}-[a-f0-9]{4}-[1-5][a-f0-9]{3}-[89ab][a-f0-9]{3}-[a-f0-9]{12}$'\n",
      "Mean weighted accuracy (instance): 0.9999999999999999\n",
      "Mean weighted accuracy (total): 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from genlm.eval import run_evaluation\n",
    "\n",
    "results = await run_evaluation(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    evaluator=evaluator,\n",
    "    max_instances=5,\n",
    "    n_replicates=1,\n",
    "    verbosity=1,\n",
    "    # output_dir=\"pattern_matching_results\", optionally save the results to a directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['average_weighted_accuracy', 'n_instances', 'all_instance_results', 'all_instance_outputs'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
