{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to SQL (Spider)\n",
    "\n",
    "This example shows how to evaluate a `genlm.control` model on the Spider domain. \n",
    "\n",
    "* **Task**: Generate SQL queries from a natural language question paired with its corresponding database schema. \n",
    "* **Data**: Development split of the Spider dataset (Yu et al., 2018). \n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the dependencies for this domain. In the root directory, run:    \n",
    "\n",
    "```bash\n",
    "pip install -e .[spider]\n",
    "```\n",
    "\n",
    "Download the punkt_tab data for nltk:\n",
    "\n",
    "```bash\n",
    "python -m nltk.downloader punkt_tab\n",
    "```\n",
    "\n",
    "To run the full spider evaluation, download the spider dataset via:\n",
    "\n",
    "```bash\n",
    "gdown 'https://drive.google.com/u/0/uc?id=1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J&export=download'\n",
    "unzip spider_data.zip\n",
    "```\n",
    "\n",
    "For this example, we'll use the `assets/spider/spider_sample` directory which contains a small subset of the spider dataset.\n",
    "\n",
    "In this example, we'll also use the grammars provided in `assets/spider/grammars.json`. This is a json file that maps each SQL schema name to a lark grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid hugginface warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage \n",
    "\n",
    "This example shows how to evaluate a `genlm.control` model on spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the dataset and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from genlm.eval.domains.spider import SpiderDataset, SpiderEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_data_dir = \"../../../assets/spider/spider_sample\"  # Replace with your path to the spider dataset\n",
    "spider_grammars = \"../../../assets/spider/grammars.json\"  # Replace with your path to the spider grammars\n",
    "\n",
    "dataset = SpiderDataset.from_spider_dir(\n",
    "    spider_data_dir, grammar_json_path=spider_grammars, few_shot_example_ids=[0, 1]\n",
    ")\n",
    "\n",
    "evaluator = SpiderEvaluator(spider_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model adaptor\n",
    "\n",
    "A model adaptor is an async callable that takes a `PatternMatchingInstance` and returns a `ModelOutput`. For this example, we'll use a constrained `genlm.control.PromptedLLM` to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from genlm.control import PromptedLLM, AWRS, BoolCFG\n",
    "from genlm.eval import ModelOutput, ModelResponse\n",
    "from genlm.eval.domains.spider import default_prompt_formatter\n",
    "\n",
    "# Load an LLM\n",
    "LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n",
    "\n",
    "\n",
    "async def model(instance, output_dir, replicate):\n",
    "    # Set the prompt for the LLM.\n",
    "    LLM.prompt_ids = default_prompt_formatter(\n",
    "        LLM.model.tokenizer, instance, use_chat_format=False\n",
    "    )\n",
    "\n",
    "    # Define a potential that ensures the generated text matches the pattern\n",
    "    potential = BoolCFG.from_lark(instance.lark_grammar).coerce(LLM, f=b\"\".join)\n",
    "\n",
    "    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n",
    "    sampler = AWRS(LLM, potential)\n",
    "\n",
    "    # Run SMC to sample sequences from the constrained model.\n",
    "    sequences = await sampler.smc(\n",
    "        n_particles=2,\n",
    "        ess_threshold=0.5,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    # Return the sampled sequences and their probabilities as a ModelOutput.\n",
    "    return ModelOutput(\n",
    "        responses=[\n",
    "            ModelResponse(response=sequence, weight=prob)\n",
    "            for sequence, prob in sequences.decoded_posterior.items()\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance utterance: How many singers do we have?, schema_name: concert_singer (id: 0)\n",
      "Mean weighted accuracy (instance): 0.9534933025699144\n",
      "Mean weighted accuracy (total): 0.9534933025699144\n",
      "\n",
      "Instance utterance: What is the total number of singers?, schema_name: concert_singer (id: 1)\n",
      "Mean weighted accuracy (instance): 0.9836474929075786\n",
      "Mean weighted accuracy (total): 0.9685703977387465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from genlm.eval import run_evaluation\n",
    "\n",
    "results = await run_evaluation(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    evaluator=evaluator,\n",
    "    max_instances=2,\n",
    "    n_replicates=1,\n",
    "    verbosity=1,\n",
    "    # output_dir=\"spider_results\", optionally save the results to a directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['average_weighted_accuracy', 'n_instances', 'all_instance_results', 'all_instance_outputs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ModelOutput(responses=[ModelResponse(response=' SELECT count(*) FROM singer', weight=0.9534933025699144, metadata=None), ModelResponse(response=' select Stadium_ID, Stadium_ID, Stadium_ID, Stadium_ID, stadium_ID, Stadium_ID, Stadium_ID + 1, Stadium_ID', weight=0.04650669743008568, metadata=None)], runtime_seconds=None, metadata=None)],\n",
       " [ModelOutput(responses=[ModelResponse(response=' SELECT count(*) FROM singer', weight=0.9836474929075786, metadata=None), ModelResponse(response=' select Stadium_ID, Stadium_ID, Stadium_ID, Stadium_ID, stadium_ID, Stadium_ID, Stadium_ID + 1, Stadium_ID', weight=0.016352507092421423, metadata=None)], runtime_seconds=None, metadata=None)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"all_instance_outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma,\n",
    "Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A\n",
    "large-scale human-labeled dataset for complex and cross-domain semantic parsing and\n",
    "text-to-SQL task. In Proceedings of the Conference on Empirical Methods in Natural Language\n",
    "Processing, 2018. URL https://aclanthology.org/D18-1425."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
