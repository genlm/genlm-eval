{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A flexible framework for evaluating constrained generation models, built for the GenLM ecosystem. This library provides standardized interfaces and benchmarks for assessing model performance across various constrained generation tasks.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started: Visit our documentation for installation and usage guides.</li> <li>API Reference: Browse the API documentation for detailed information about the library's components.</li> <li>Cookbook: Check out our examples and tutorials for:<ul> <li>Using built-in domains (Pattern Matching, Text-to-SQL, Molecular Synthesis)</li> <li>Creating custom evaluation domains</li> </ul> </li> </ul>"},{"location":"#components","title":"Components","text":"<ul> <li>Datasets: Specifies and iterates over the dataset instances of a constrained generation task.</li> <li>Evaluators: Evaluates the model's output.</li> <li>Model Adapters: Wraps the model to provide a unified interface for evaluation.</li> <li>Runners: Orchestrates the evaluation process with output caching.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Note: This library is still under active development.</p> <pre><code>git clone https://github.com/genlm/genlm-eval.git\ncd genlm-eval\npip install -e .\n</code></pre> <p>For domain-specific dependencies, refer to the cookbook in the docs.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport shutil\nimport mkdocs_gen_files\n</pre> from pathlib import Path import shutil import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>readme = Path(\"README.md\")\nindex_md = Path(\"docs/index.md\")\nlogo = Path(\"assets/logo.png\")\n</pre> readme = Path(\"README.md\") index_md = Path(\"docs/index.md\") logo = Path(\"assets/logo.png\") In\u00a0[\u00a0]: Copied! <pre>def files_are_different(src: Path, dst: Path) -&gt; bool:\n    if not dst.exists():\n        return True\n    return src.read_bytes() != dst.read_bytes()\n</pre> def files_are_different(src: Path, dst: Path) -&gt; bool:     if not dst.exists():         return True     return src.read_bytes() != dst.read_bytes() In\u00a0[\u00a0]: Copied! <pre>if readme.exists() and files_are_different(readme, index_md):\n    shutil.copyfile(readme, index_md)\n</pre> if readme.exists() and files_are_different(readme, index_md):     shutil.copyfile(readme, index_md) In\u00a0[\u00a0]: Copied! <pre>if logo.exists() and files_are_different(logo, Path(\"docs/assets/logo.png\")):\n    shutil.copyfile(logo, \"docs/assets/logo.png\")\n</pre> if logo.exists() and files_are_different(logo, Path(\"docs/assets/logo.png\")):     shutil.copyfile(logo, \"docs/assets/logo.png\") In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"genlm/eval\").rglob(\"*.py\")):\n    if any(part.startswith(\".\") for part in path.parts):\n        continue\n\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\".\").with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        print(f\"init, making parts {parts[:-1]}\")\n        parts = parts[:-1]\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"::: {ident}\\n\")\n        fd.write(\"    options:\\n\")\n        fd.write(\"      show_root_heading: true\\n\")\n        fd.write(\"      show_source: true\\n\")\n        fd.write(\"      heading_level: 2\\n\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\"genlm/eval\").rglob(\"*.py\")):     if any(part.startswith(\".\") for part in path.parts):         continue      module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\".\").with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)      if parts[-1] == \"__init__\":         print(f\"init, making parts {parts[:-1]}\")         parts = parts[:-1]     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"::: {ident}\\n\")         fd.write(\"    options:\\n\")         fd.write(\"      show_root_heading: true\\n\")         fd.write(\"      show_source: true\\n\")         fd.write(\"      heading_level: 2\\n\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"cookbook/cookbook/","title":"Cookbook","text":"<p>This cookbook provides examples of how to use <code>genlm-eval</code> for evaluating constrained language models on various domains. Each example demonstrates how to:</p> <ol> <li>Set up the required dependencies</li> <li>Initialize the dataset and evaluator</li> <li>Define a model adaptor</li> <li>Run the evaluation</li> </ol>"},{"location":"cookbook/cookbook/#available-examples","title":"Available Examples","text":""},{"location":"cookbook/cookbook/#custom-domains","title":"Custom Domains","text":"<p>Learn how to extend <code>genlm-eval</code> to evaluate models on your own custom domains. This example walks through:</p> <ul> <li>Defining a dataset schema</li> <li>Implementing an evaluator</li> <li>Creating a model adaptor</li> <li>Running the evaluation pipeline</li> </ul> <p>View Example</p>"},{"location":"cookbook/cookbook/#domain-specific-examples","title":"Domain-Specific Examples","text":""},{"location":"cookbook/cookbook/#pattern-matching","title":"Pattern Matching","text":"<p>Evaluate models on generating strings that match complex pattern specifications:</p> <ul> <li>Task: Generate strings conforming to expressive pattern-matching specifications</li> <li>Data: 400+ pattern-matching specifications with features beyond regular expressions</li> </ul> <p>View Example</p>"},{"location":"cookbook/cookbook/#molecular-synthesis","title":"Molecular Synthesis","text":"<p>Evaluate models on generating valid molecular structures:</p> <ul> <li>Task: Generate drug-like compounds using SMILES notation</li> <li>Data: Few-shot prompts from GDB-17 database</li> </ul> <p>View Example</p>"},{"location":"cookbook/cookbook/#text-to-sql-spider","title":"Text to SQL (Spider)","text":"<p>Evaluate models on generating SQL queries from natural language:</p> <ul> <li>Task: Generate SQL queries from natural language questions</li> <li>Data: Spider dataset with database schemas</li> </ul> <p>View Example</p>"},{"location":"cookbook/custom_domains/","title":"Custom Domains","text":"In\u00a0[1]: Copied! <pre>from genlm.eval import Instance\n\n\nclass PatternMatchingInstance(Instance):\n    \"\"\"Schema for a pattern matching instance.\"\"\"\n\n    pattern: str\n    instance_id: int\n\n    def __repr__(self):\n        return f\"pattern: {self.pattern} (id: {self.instance_id})\"\n</pre> from genlm.eval import Instance   class PatternMatchingInstance(Instance):     \"\"\"Schema for a pattern matching instance.\"\"\"      pattern: str     instance_id: int      def __repr__(self):         return f\"pattern: {self.pattern} (id: {self.instance_id})\" <p>Given a dataset schema, you can define a dataset by subclassing <code>Dataset</code> and implementing an <code>__iter__</code> method which yields instances of the schema.</p> In\u00a0[2]: Copied! <pre>from genlm.eval import Dataset\n\n\nclass PatternMatchingDataset(Dataset[PatternMatchingInstance]):\n    \"\"\"Dataset for pattern matching evaluation.\"\"\"\n\n    def __init__(self, patterns):\n        self.patterns = patterns\n\n    def __iter__(self):\n        \"\"\"Iterate over regex patterns.\n\n        Returns:\n            (Iterator[PatternMatchingInstance]): Iterator over regex instances.\n        \"\"\"\n        for pattern_id, pattern in enumerate(self.patterns):\n            yield PatternMatchingInstance(pattern=pattern, instance_id=pattern_id)\n\n    @property\n    def schema(self):\n        \"\"\"Get the schema class for this dataset.\"\"\"\n        return PatternMatchingInstance\n</pre> from genlm.eval import Dataset   class PatternMatchingDataset(Dataset[PatternMatchingInstance]):     \"\"\"Dataset for pattern matching evaluation.\"\"\"      def __init__(self, patterns):         self.patterns = patterns      def __iter__(self):         \"\"\"Iterate over regex patterns.          Returns:             (Iterator[PatternMatchingInstance]): Iterator over regex instances.         \"\"\"         for pattern_id, pattern in enumerate(self.patterns):             yield PatternMatchingInstance(pattern=pattern, instance_id=pattern_id)      @property     def schema(self):         \"\"\"Get the schema class for this dataset.\"\"\"         return PatternMatchingInstance In\u00a0[3]: Copied! <pre>import regex\nfrom genlm.eval import Evaluator, EvaluationResult\n\n\nclass PatternMatchingEvaluator(Evaluator[PatternMatchingInstance]):\n    \"\"\"Evaluator for pattern matching.\"\"\"\n\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate if a response matches the regex pattern.\"\"\"\n        is_valid = regex.compile(instance.pattern).fullmatch(response) is not None\n        return EvaluationResult(\n            score=int(is_valid), desc=\"valid\" if is_valid else \"invalid\"\n        )\n</pre> import regex from genlm.eval import Evaluator, EvaluationResult   class PatternMatchingEvaluator(Evaluator[PatternMatchingInstance]):     \"\"\"Evaluator for pattern matching.\"\"\"      def evaluate_sample(self, instance, response):         \"\"\"Evaluate if a response matches the regex pattern.\"\"\"         is_valid = regex.compile(instance.pattern).fullmatch(response) is not None         return EvaluationResult(             score=int(is_valid), desc=\"valid\" if is_valid else \"invalid\"         ) In\u00a0[4]: Copied! <pre>from genlm.control import PromptedLLM, AWRS\nfrom genlm.eval import ModelOutput, ModelResponse\nfrom genlm.eval.domains.pattern_matching import (\n    default_prompt_formatter,\n    PatternPotential,\n)\n\n# Load an LLM\nLLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n\n\nasync def model(instance, output_dir, replicate):\n    # Set the prompt for the LLM.\n    LLM.prompt_ids = default_prompt_formatter(\n        LLM.model.tokenizer, instance, use_chat_format=False\n    )\n\n    # Define a potential that ensures the generated text matches the pattern\n    potential = PatternPotential(instance.pattern).coerce(LLM, f=b\"\".join)\n\n    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n    sampler = AWRS(LLM, potential)\n\n    # Run SMC to sample sequences from the constrained model.\n    sequences = await sampler.smc(\n        n_particles=5,\n        ess_threshold=0.5,\n        max_tokens=100,\n    )\n\n    # Return the sampled sequences and their probabilities as a ModelOutput.\n    return ModelOutput(\n        responses=[\n            ModelResponse(response=sequence, weight=prob)\n            for sequence, prob in sequences.decoded_posterior.items()\n        ],\n    )\n</pre> from genlm.control import PromptedLLM, AWRS from genlm.eval import ModelOutput, ModelResponse from genlm.eval.domains.pattern_matching import (     default_prompt_formatter,     PatternPotential, )  # Load an LLM LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])   async def model(instance, output_dir, replicate):     # Set the prompt for the LLM.     LLM.prompt_ids = default_prompt_formatter(         LLM.model.tokenizer, instance, use_chat_format=False     )      # Define a potential that ensures the generated text matches the pattern     potential = PatternPotential(instance.pattern).coerce(LLM, f=b\"\".join)      # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.     sampler = AWRS(LLM, potential)      # Run SMC to sample sequences from the constrained model.     sequences = await sampler.smc(         n_particles=5,         ess_threshold=0.5,         max_tokens=100,     )      # Return the sampled sequences and their probabilities as a ModelOutput.     return ModelOutput(         responses=[             ModelResponse(response=sequence, weight=prob)             for sequence, prob in sequences.decoded_posterior.items()         ],     ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n  warnings.warn(\n</pre> In\u00a0[6]: Copied! <pre>from genlm.eval import run_evaluation\n\ndataset = PatternMatchingDataset([r\"xy|xz\", r\"ab|c(e|f)\"])\nevaluator = PatternMatchingEvaluator()\n\nresults = await run_evaluation(\n    dataset=dataset,\n    evaluator=evaluator,\n    model=model,\n    n_replicates=1,\n    verbosity=1,\n    # output_dir=\"results\", # uncomment to save results\n)\n</pre> from genlm.eval import run_evaluation  dataset = PatternMatchingDataset([r\"xy|xz\", r\"ab|c(e|f)\"]) evaluator = PatternMatchingEvaluator()  results = await run_evaluation(     dataset=dataset,     evaluator=evaluator,     model=model,     n_replicates=1,     verbosity=1,     # output_dir=\"results\", # uncomment to save results ) <pre>Instance instance_id=0 pattern='xy|xz'\nMean weighted accuracy (instance): 0.9999999999999999\nMean weighted accuracy (total): 0.9999999999999999\n\nInstance instance_id=1 pattern='ab|c(e|f)'\nMean weighted accuracy (instance): 1.0\nMean weighted accuracy (total): 1.0\n\n</pre>"},{"location":"cookbook/custom_domains/#custom-domains","title":"Custom Domains\u00b6","text":"<p>This library is designed to be extensible to new domains. To evaluate a model on a custom domain, you need to:</p> <ol> <li>Define your dataset</li> <li>Implement an evaluator</li> <li>Implement a model adaptor</li> </ol> <p>The following example demonstrates these steps on the pattern matching domain.</p>"},{"location":"cookbook/custom_domains/#1-define-your-dataset","title":"1. Define your dataset\u00b6","text":"<p>A dataset is an iterator over dataset instances satisfying a schema. The schema is defined by a class that inherits from <code>Instance</code>.</p>"},{"location":"cookbook/custom_domains/#2-implement-an-evaluator","title":"2. Implement an evaluator\u00b6","text":"<p>An evaluator is the class responsible for scoring model outputs. Subclasses must minimally implement the <code>evaluate_sample</code> method which takes an instance and a response and returns an evaluation result.</p>"},{"location":"cookbook/custom_domains/#3-implement-a-model-adaptor","title":"3. Implement a model adaptor\u00b6","text":"<p>A model adaptor is an async callable that takes a <code>PatternMatchingInstance</code> and returns a <code>ModelOutput</code>. For this example, we'll use a constrained <code>genlm.control.PromptedLLM</code> to generate responses.</p>"},{"location":"cookbook/custom_domains/#4-run-the-evaluation","title":"4. Run the evaluation\u00b6","text":"<p>Using the dataset, evaluator, and model adaptor, we can now run the evaluation:</p>"},{"location":"cookbook/domains/molecular_synthesis/","title":"Molecular Synthesis","text":"In\u00a0[1]: Copied! <pre>from genlm.eval.domains.molecular_synthesis import (\n    MolecularSynthesisDataset,\n    MolecularSynthesisEvaluator,\n)\n</pre> from genlm.eval.domains.molecular_synthesis import (     MolecularSynthesisDataset,     MolecularSynthesisEvaluator, ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Sample 5 instances each with 5 molecules to use as few-shot examples\ndataset = MolecularSynthesisDataset.from_smiles(\n    \"../../../assets/molecular_synthesis/GDB17_sample.txt\", n_molecules=5, n_instances=5\n)\n\nevaluator = MolecularSynthesisEvaluator()\n</pre> # Sample 5 instances each with 5 molecules to use as few-shot examples dataset = MolecularSynthesisDataset.from_smiles(     \"../../../assets/molecular_synthesis/GDB17_sample.txt\", n_molecules=5, n_instances=5 )  evaluator = MolecularSynthesisEvaluator() In\u00a0[3]: Copied! <pre>from genlm.control import PromptedLLM, AWRS\nfrom genlm.eval import ModelOutput, ModelResponse\nfrom genlm.eval.domains.molecular_synthesis import (\n    default_prompt_formatter,\n    PartialSMILES,\n)\n\n# Load an LLM\nLLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n\n\nasync def model(instance, output_dir, replicate):\n    # Set the prompt for the LLM.\n    LLM.prompt_ids = default_prompt_formatter(\n        LLM.model.tokenizer, instance, use_chat_format=False\n    )\n\n    # Define a potential that ensures the generated molecules are valid SMILES\n    potential = PartialSMILES().coerce(LLM, f=b\"\".join)\n\n    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n    sampler = AWRS(LLM, potential)\n\n    # Run SMC to sample sequences from the constrained model.\n    sequences = await sampler.smc(\n        n_particles=5,\n        ess_threshold=0.5,\n        max_tokens=100,\n    )\n\n    # Return the sampled sequences and their probabilities as a ModelOutput.\n    return ModelOutput(\n        responses=[\n            ModelResponse(response=sequence, weight=prob)\n            for sequence, prob in sequences.decoded_posterior.items()\n        ],\n    )\n</pre> from genlm.control import PromptedLLM, AWRS from genlm.eval import ModelOutput, ModelResponse from genlm.eval.domains.molecular_synthesis import (     default_prompt_formatter,     PartialSMILES, )  # Load an LLM LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])   async def model(instance, output_dir, replicate):     # Set the prompt for the LLM.     LLM.prompt_ids = default_prompt_formatter(         LLM.model.tokenizer, instance, use_chat_format=False     )      # Define a potential that ensures the generated molecules are valid SMILES     potential = PartialSMILES().coerce(LLM, f=b\"\".join)      # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.     sampler = AWRS(LLM, potential)      # Run SMC to sample sequences from the constrained model.     sequences = await sampler.smc(         n_particles=5,         ess_threshold=0.5,         max_tokens=100,     )      # Return the sampled sequences and their probabilities as a ModelOutput.     return ModelOutput(         responses=[             ModelResponse(response=sequence, weight=prob)             for sequence, prob in sequences.decoded_posterior.items()         ],     ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>from genlm.eval import run_evaluation\n\nresults = await run_evaluation(\n    dataset=dataset,\n    model=model,\n    evaluator=evaluator,\n    max_instances=2,\n    n_replicates=1,\n    verbosity=1,\n    # output_dir=\"molecular_synthesis_results\", optionally save the results to a directory\n)\n</pre> from genlm.eval import run_evaluation  results = await run_evaluation(     dataset=dataset,     model=model,     evaluator=evaluator,     max_instances=2,     n_replicates=1,     verbosity=1,     # output_dir=\"molecular_synthesis_results\", optionally save the results to a directory ) <pre>Instance instance_id=0 molecules=['BrC1=C2C3CC33C(NCS3(=O)=O)C2=CC=C1\\n', 'BrC1=C2C3C4COC(=NCC2=NSC1=O)C34\\n', 'BrC1=C2C3=C4C(CC3CCC2=O)C(=N)NC4=N1\\n', 'BrC1=C2C3C4C3N(CC4C#C)C2=NC(=O)S1\\n', 'BrC1=C2C3C4CC(C3CC2=NC(=N)O1)C(=O)O4\\n']\nMean weighted accuracy (instance): 0.6121801531912207\nMean weighted accuracy (total): 0.6121801531912207\n\nInstance instance_id=1 molecules=['BrC1=C2C3CC3C=CCC#CC1=CSC2=N', 'BrC1=C2C3CC3C3=C(C=NS3)N2C(=N)C=N1\\n', 'BrC1=C2C3C4NC4C(C3C#C)C2=NSC1=O\\n', 'BrC1=C2C3C4CC4C(C3C=O)C2=NNS1(=O)=O\\n', 'BrC1=C2C3C4NC=NC4C3OC2=CSC1=N\\n']\nMean weighted accuracy (instance): 0.0\nMean weighted accuracy (total): 0.3060900765956103\n\n</pre> In\u00a0[5]: Copied! <pre>results.keys()\n</pre> results.keys() Out[5]: <pre>dict_keys(['average_weighted_accuracy', 'n_instances', 'all_instance_results', 'all_instance_outputs'])</pre>"},{"location":"cookbook/domains/molecular_synthesis/#molecular-synthesis","title":"Molecular Synthesis\u00b6","text":"<p>This example shows how to evaluate a <code>genlm.control</code> model on the molecular synthesis domain.</p> <ul> <li>Task: Produce drug-like compounds using the SMILES notation (Weininger, 1988).</li> <li>Data: Few-shot prompts created by repeatedly selecting 20 random samples from the GDB-17 database (Ruddigkeit et al., 2012).</li> </ul>"},{"location":"cookbook/domains/molecular_synthesis/#setup","title":"Setup\u00b6","text":"<p>First, install the dependencies for this domain. In the root directory, run:</p> <pre>pip install -e .[molecules]\n</pre> <p>Second, download the <code>GDB17_sample.txt</code> file, which contains 30 molecules.</p> <p>This file is taken from the GDB17 dataset, which can be downloaded from https://gdb.unibe.ch/downloads/. For a full evaluation, download the GDB-17-Set (50 million) file.</p>"},{"location":"cookbook/domains/molecular_synthesis/#usage","title":"Usage\u00b6","text":""},{"location":"cookbook/domains/molecular_synthesis/#initialize-the-dataset-and-evaluator","title":"Initialize the dataset and evaluator\u00b6","text":""},{"location":"cookbook/domains/molecular_synthesis/#define-a-model-adaptor","title":"Define a model adaptor\u00b6","text":"<p>A model adaptor is an async callable that takes a <code>PatternMatchingInstance</code> and returns a <code>ModelOutput</code>. For this example, we'll use a <code>genlm.control.PromptedLLM</code> constrained to generate valid SMILES (via the <code>PartialSMILES</code> potential) to generate responses.</p>"},{"location":"cookbook/domains/molecular_synthesis/#run-the-evaluation","title":"Run the evaluation\u00b6","text":""},{"location":"cookbook/domains/molecular_synthesis/#references","title":"References\u00b6","text":"<p>Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enu- meration of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of chemical information and modeling, 52(11):2864\u20132875, 2012. URL https://pubs.acs.org/doi/pdf/10.1021/ci300415d.</p> <p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31\u201336, 1988.</p>"},{"location":"cookbook/domains/pattern_matching/","title":"Pattern Matching","text":"In\u00a0[1]: Copied! <pre>from genlm.eval.domains.pattern_matching import (\n    PatternMatchingDataset,\n    PatternMatchingEvaluator,\n)\n</pre> from genlm.eval.domains.pattern_matching import (     PatternMatchingDataset,     PatternMatchingEvaluator, ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>dataset = PatternMatchingDataset.from_csv(\n    \"../../../assets/pattern_matching/patterns.csv\", pattern_column=\"regex\"\n)\n\nevaluator = PatternMatchingEvaluator()\n</pre> dataset = PatternMatchingDataset.from_csv(     \"../../../assets/pattern_matching/patterns.csv\", pattern_column=\"regex\" )  evaluator = PatternMatchingEvaluator() In\u00a0[3]: Copied! <pre>from genlm.control import PromptedLLM, AWRS\nfrom genlm.eval import ModelOutput, ModelResponse\nfrom genlm.eval.domains.pattern_matching import (\n    default_prompt_formatter,\n    PatternPotential,\n)\n\n# Load an LLM\nLLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n\n\nasync def model(instance, output_dir, replicate):\n    # Set the prompt for the LLM.\n    LLM.prompt_ids = default_prompt_formatter(\n        LLM.model.tokenizer, instance, use_chat_format=False\n    )\n\n    # Define a potential that ensures the generated text matches the pattern\n    potential = PatternPotential(instance.pattern).coerce(LLM, f=b\"\".join)\n\n    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n    sampler = AWRS(LLM, potential)\n\n    # Run SMC to sample sequences from the constrained model.\n    sequences = await sampler.smc(\n        n_particles=5,\n        ess_threshold=0.5,\n        max_tokens=100,\n    )\n\n    # Return the sampled sequences and their probabilities as a ModelOutput.\n    return ModelOutput(\n        responses=[\n            ModelResponse(response=sequence, weight=prob)\n            for sequence, prob in sequences.decoded_posterior.items()\n        ],\n    )\n</pre> from genlm.control import PromptedLLM, AWRS from genlm.eval import ModelOutput, ModelResponse from genlm.eval.domains.pattern_matching import (     default_prompt_formatter,     PatternPotential, )  # Load an LLM LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])   async def model(instance, output_dir, replicate):     # Set the prompt for the LLM.     LLM.prompt_ids = default_prompt_formatter(         LLM.model.tokenizer, instance, use_chat_format=False     )      # Define a potential that ensures the generated text matches the pattern     potential = PatternPotential(instance.pattern).coerce(LLM, f=b\"\".join)      # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.     sampler = AWRS(LLM, potential)      # Run SMC to sample sequences from the constrained model.     sequences = await sampler.smc(         n_particles=5,         ess_threshold=0.5,         max_tokens=100,     )      # Return the sampled sequences and their probabilities as a ModelOutput.     return ModelOutput(         responses=[             ModelResponse(response=sequence, weight=prob)             for sequence, prob in sequences.decoded_posterior.items()         ],     ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>from genlm.eval import run_evaluation\n\nresults = await run_evaluation(\n    dataset=dataset,\n    model=model,\n    evaluator=evaluator,\n    max_instances=5,\n    n_replicates=1,\n    verbosity=1,\n    # output_dir=\"pattern_matching_results\", optionally save the results to a directory\n)\n</pre> from genlm.eval import run_evaluation  results = await run_evaluation(     dataset=dataset,     model=model,     evaluator=evaluator,     max_instances=5,     n_replicates=1,     verbosity=1,     # output_dir=\"pattern_matching_results\", optionally save the results to a directory ) <pre>Instance instance_id=0 pattern='(?&lt;!\\\\d{3})abc(?!\\\\d{3})'\nMean weighted accuracy (instance): 1.0\nMean weighted accuracy (total): 1.0\n\nInstance instance_id=1 pattern='^(?|(a)|(b)|(c))\\\\1$'\nMean weighted accuracy (instance): 1.0\nMean weighted accuracy (total): 1.0\n\nInstance instance_id=2 pattern='[\\\\p{IsAlphabetic}&amp;&amp;[\\\\P{L}]]'\nMean weighted accuracy (instance): 1.0\nMean weighted accuracy (total): 1.0\n\nInstance instance_id=3 pattern='^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\nMean weighted accuracy (instance): 1.0\nMean weighted accuracy (total): 1.0\n\nInstance instance_id=4 pattern='^[a-f0-9]{8}-[a-f0-9]{4}-[1-5][a-f0-9]{3}-[89ab][a-f0-9]{3}-[a-f0-9]{12}$'\nMean weighted accuracy (instance): 0.9999999999999999\nMean weighted accuracy (total): 1.0\n\n</pre> In\u00a0[5]: Copied! <pre>results.keys()\n</pre> results.keys() Out[5]: <pre>dict_keys(['average_weighted_accuracy', 'n_instances', 'all_instance_results', 'all_instance_outputs'])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"cookbook/domains/pattern_matching/#pattern-matching","title":"Pattern Matching\u00b6","text":"<p>This example shows how to evaluate a <code>genlm.control</code> model on the pattern matching domain.</p> <ul> <li>Task: Generate strings that conform to expressive pattern-matching specifications. Compared to formal regular expressions, these patterns contain explicit features that cannot be fully captured by deterministic finite-state automata, including unbounded center embedding and conditionals.</li> <li>Data: Over 400 pattern-matching specifications generated via the pipeline described in Appendix I of (Lipkin et al., 2025).</li> </ul>"},{"location":"cookbook/domains/pattern_matching/#setup","title":"Setup\u00b6","text":"<p>First, install the dependencies for this domain. In the root directory, run:</p> <pre>pip install -e .[pattern_matching]\n</pre> <p>Second, download the <code>patterns.csv</code> file from the <code>assets/pattern_matching</code> directory in the repository. (Note that you can also use your own patterns.)</p>"},{"location":"cookbook/domains/pattern_matching/#usage","title":"Usage\u00b6","text":"<p>This example shows how to evaluate a <code>genlm.control</code> model on the pattern matching domain.</p>"},{"location":"cookbook/domains/pattern_matching/#initialize-the-dataset-and-evaluator","title":"Initialize the dataset and evaluator\u00b6","text":""},{"location":"cookbook/domains/pattern_matching/#define-a-model-adaptor","title":"Define a model adaptor\u00b6","text":"<p>A model adaptor is an async callable that takes a <code>PatternMatchingInstance</code> and returns a <code>ModelOutput</code>. For this example, we'll use a constrained <code>genlm.control.PromptedLLM</code> to generate responses.</p>"},{"location":"cookbook/domains/pattern_matching/#run-the-evaluation","title":"Run the evaluation\u00b6","text":""},{"location":"cookbook/domains/spider/","title":"Text to SQL (Spider)","text":"In\u00a0[1]: Copied! <pre>import os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid hugginface warnings\n</pre> import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid hugginface warnings In\u00a0[2]: Copied! <pre>from genlm.eval.domains.spider import SpiderDataset, SpiderEvaluator\n</pre> from genlm.eval.domains.spider import SpiderDataset, SpiderEvaluator <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>spider_data_dir = \"../../../assets/spider/spider_sample\"  # Replace with your path to the spider dataset\nspider_grammars = \"../../../assets/spider/grammars.json\"  # Replace with your path to the spider grammars\n\ndataset = SpiderDataset.from_spider_dir(\n    spider_data_dir, grammar_json_path=spider_grammars, few_shot_example_ids=[0, 1]\n)\n\nevaluator = SpiderEvaluator(spider_data_dir)\n</pre> spider_data_dir = \"../../../assets/spider/spider_sample\"  # Replace with your path to the spider dataset spider_grammars = \"../../../assets/spider/grammars.json\"  # Replace with your path to the spider grammars  dataset = SpiderDataset.from_spider_dir(     spider_data_dir, grammar_json_path=spider_grammars, few_shot_example_ids=[0, 1] )  evaluator = SpiderEvaluator(spider_data_dir) In\u00a0[4]: Copied! <pre>from genlm.control import PromptedLLM, AWRS, BoolCFG\nfrom genlm.eval import ModelOutput, ModelResponse\nfrom genlm.eval.domains.spider import default_prompt_formatter\n\n# Load an LLM\nLLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])\n\n\nasync def model(instance, output_dir, replicate):\n    # Set the prompt for the LLM.\n    LLM.prompt_ids = default_prompt_formatter(\n        LLM.model.tokenizer, instance, use_chat_format=False\n    )\n\n    # Define a potential that ensures the generated text matches the pattern\n    potential = BoolCFG.from_lark(instance.lark_grammar).coerce(LLM, f=b\"\".join)\n\n    # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.\n    sampler = AWRS(LLM, potential)\n\n    # Run SMC to sample sequences from the constrained model.\n    sequences = await sampler.smc(\n        n_particles=2,\n        ess_threshold=0.5,\n        max_tokens=100,\n    )\n\n    # Return the sampled sequences and their probabilities as a ModelOutput.\n    return ModelOutput(\n        responses=[\n            ModelResponse(response=sequence, weight=prob)\n            for sequence, prob in sequences.decoded_posterior.items()\n        ],\n    )\n</pre> from genlm.control import PromptedLLM, AWRS, BoolCFG from genlm.eval import ModelOutput, ModelResponse from genlm.eval.domains.spider import default_prompt_formatter  # Load an LLM LLM = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\", b\"\\n\\n\"])   async def model(instance, output_dir, replicate):     # Set the prompt for the LLM.     LLM.prompt_ids = default_prompt_formatter(         LLM.model.tokenizer, instance, use_chat_format=False     )      # Define a potential that ensures the generated text matches the pattern     potential = BoolCFG.from_lark(instance.lark_grammar).coerce(LLM, f=b\"\".join)      # Define an adaptive weighted rejection sampler to sample tokens from the constrained model.     sampler = AWRS(LLM, potential)      # Run SMC to sample sequences from the constrained model.     sequences = await sampler.smc(         n_particles=2,         ess_threshold=0.5,         max_tokens=100,     )      # Return the sampled sequences and their probabilities as a ModelOutput.     return ModelOutput(         responses=[             ModelResponse(response=sequence, weight=prob)             for sequence, prob in sequences.decoded_posterior.items()         ],     ) <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre>from genlm.eval import run_evaluation\n\nresults = await run_evaluation(\n    dataset=dataset,\n    model=model,\n    evaluator=evaluator,\n    max_instances=2,\n    n_replicates=1,\n    verbosity=1,\n    # output_dir=\"spider_results\", optionally save the results to a directory\n)\n</pre> from genlm.eval import run_evaluation  results = await run_evaluation(     dataset=dataset,     model=model,     evaluator=evaluator,     max_instances=2,     n_replicates=1,     verbosity=1,     # output_dir=\"spider_results\", optionally save the results to a directory ) <pre>Instance utterance: How many singers do we have?, schema_name: concert_singer (id: 0)\nMean weighted accuracy (instance): 0.9534933025699144\nMean weighted accuracy (total): 0.9534933025699144\n\nInstance utterance: What is the total number of singers?, schema_name: concert_singer (id: 1)\nMean weighted accuracy (instance): 0.9836474929075786\nMean weighted accuracy (total): 0.9685703977387465\n\n</pre> In\u00a0[6]: Copied! <pre>results.keys()\n</pre> results.keys() Out[6]: <pre>dict_keys(['average_weighted_accuracy', 'n_instances', 'all_instance_results', 'all_instance_outputs'])</pre> In\u00a0[7]: Copied! <pre>results[\"all_instance_outputs\"]\n</pre> results[\"all_instance_outputs\"] Out[7]: <pre>[[ModelOutput(responses=[ModelResponse(response=' SELECT count(*) FROM singer', weight=0.9534933025699144, metadata=None), ModelResponse(response=' select Stadium_ID, Stadium_ID, Stadium_ID, Stadium_ID, stadium_ID, Stadium_ID, Stadium_ID + 1, Stadium_ID', weight=0.04650669743008568, metadata=None)], runtime_seconds=None, metadata=None)],\n [ModelOutput(responses=[ModelResponse(response=' SELECT count(*) FROM singer', weight=0.9836474929075786, metadata=None), ModelResponse(response=' select Stadium_ID, Stadium_ID, Stadium_ID, Stadium_ID, stadium_ID, Stadium_ID, Stadium_ID + 1, Stadium_ID', weight=0.016352507092421423, metadata=None)], runtime_seconds=None, metadata=None)]]</pre>"},{"location":"cookbook/domains/spider/#text-to-sql-spider","title":"Text to SQL (Spider)\u00b6","text":"<p>This example shows how to evaluate a <code>genlm.control</code> model on the Spider domain.</p> <ul> <li>Task: Generate SQL queries from a natural language question paired with its corresponding database schema.</li> <li>Data: Development split of the Spider dataset (Yu et al., 2018).</li> </ul>"},{"location":"cookbook/domains/spider/#setup","title":"Setup\u00b6","text":"<p>First, install the dependencies for this domain. In the root directory, run:</p> <pre>pip install -e .[spider]\n</pre> <p>Download the punkt_tab data for nltk:</p> <pre>python -m nltk.downloader punkt_tab\n</pre> <p>To run the full spider evaluation, download the spider dataset via:</p> <pre>gdown 'https://drive.google.com/u/0/uc?id=1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J&amp;export=download'\nunzip spider_data.zip\n</pre> <p>For this example, we'll use the <code>assets/spider/spider_sample</code> directory which contains a small subset of the spider dataset.</p> <p>In this example, we'll also use the grammars provided in <code>assets/spider/grammars.json</code>. This is a json file that maps each SQL schema name to a lark grammar.</p>"},{"location":"cookbook/domains/spider/#usage","title":"Usage\u00b6","text":"<p>This example shows how to evaluate a <code>genlm.control</code> model on spider.</p>"},{"location":"cookbook/domains/spider/#initialize-the-dataset-and-evaluator","title":"Initialize the dataset and evaluator\u00b6","text":""},{"location":"cookbook/domains/spider/#define-a-model-adaptor","title":"Define a model adaptor\u00b6","text":"<p>A model adaptor is an async callable that takes a <code>PatternMatchingInstance</code> and returns a <code>ModelOutput</code>. For this example, we'll use a constrained <code>genlm.control.PromptedLLM</code> to generate responses.</p>"},{"location":"cookbook/domains/spider/#run-the-evaluation","title":"Run the evaluation\u00b6","text":""},{"location":"cookbook/domains/spider/#references","title":"References\u00b6","text":"<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.org/D18-1425.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>genlm<ul> <li>eval<ul> <li>core<ul> <li>dataset</li> <li>evaluator</li> <li>model</li> <li>runner</li> </ul> </li> <li>domains<ul> <li>molecular_synthesis</li> <li>pattern_matching</li> <li>spider<ul> <li>spider</li> <li>spider_eval<ul> <li>content_encoder</li> <li>dialogue</li> <li>evaluation</li> <li>evaluator</li> <li>paths</li> <li>process_sql</li> <li>schema</li> <li>utils</li> </ul> </li> <li>table_column_potential</li> </ul> </li> </ul> </li> <li>util</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/genlm/eval/__init__/","title":"eval","text":""},{"location":"reference/genlm/eval/__init__/#genlm.eval","title":"<code>genlm.eval</code>","text":""},{"location":"reference/genlm/eval/__init__/#genlm.eval.Instance","title":"<code>Instance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for dataset instances that conform to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Instance(BaseModel):\n    \"\"\"Base class for dataset instances that conform to a Pydantic schema.\"\"\"\n\n    instance_id: Union[int, str]\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for datasets that yield instances conforming to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Dataset(Generic[T], ABC):\n    \"\"\"Base class for datasets that yield instances conforming to a Pydantic schema.\"\"\"\n\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[T]:\n        \"\"\"Iterate over dataset instances.\n\n        Returns:\n            Iterator[T]: An iterator over instances conforming to schema T.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def schema(self) -&gt; type[T]:\n        \"\"\"Get the Pydantic schema class for this dataset.\n\n        Returns:\n            type[T]: The Pydantic model class defining the schema.\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over dataset instances.</p> <p>Returns:</p> Type Description <code>Iterator[T]</code> <p>Iterator[T]: An iterator over instances conforming to schema T.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Iterator[T]:\n    \"\"\"Iterate over dataset instances.\n\n    Returns:\n        Iterator[T]: An iterator over instances conforming to schema T.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Dataset.schema","title":"<code>schema</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the Pydantic schema class for this dataset.</p> <p>Returns:</p> Type Description <code>type[T]</code> <p>type[T]: The Pydantic model class defining the schema.</p>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Evaluator","title":"<code>Evaluator</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for evaluators that handle response evaluation.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class Evaluator(Generic[T], ABC):\n    \"\"\"Base class for evaluators that handle response evaluation.\"\"\"\n\n    @abstractmethod\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate a single response for correctness.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n        Returns:\n            (EvaluationResult): The evaluation result.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            output (ModelOutput): The complete model output including ensemble responses.\n\n        Returns:\n            (Dict[str, Any]): Dictionary containing evaluation metrics.\n        \"\"\"\n        weighted_accuracy = 0.0\n        results = []\n        for response in output.responses:\n            result = self.evaluate_sample(instance, response.response)\n            weighted_accuracy += result.score * response.weight\n            results.append(\n                {\n                    \"score\": result.score,\n                    \"desc\": result.desc,\n                    \"metadata\": result.metadata,\n                }\n            )\n\n        return {\n            \"weighted_accuracy\": weighted_accuracy,\n            \"runtime_seconds\": output.runtime_seconds,\n            \"results\": results,\n        }\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Evaluator.evaluate_sample","title":"<code>evaluate_sample(instance, response)</code>  <code>abstractmethod</code>","text":"<p>Evaluate a single response for correctness.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>response</code> <code>Any</code> <p>The model's response, which is given by the response attribute of a <code>ModelOutput</code> object.</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>The evaluation result.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>@abstractmethod\ndef evaluate_sample(self, instance, response):\n    \"\"\"Evaluate a single response for correctness.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n    Returns:\n        (EvaluationResult): The evaluation result.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.Evaluator.evaluate_ensemble","title":"<code>evaluate_ensemble(instance, output)</code>","text":"<p>Evaluate the complete ensemble of weighted samples using weighted accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>output</code> <code>ModelOutput</code> <p>The complete model output including ensemble responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing evaluation metrics.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        output (ModelOutput): The complete model output including ensemble responses.\n\n    Returns:\n        (Dict[str, Any]): Dictionary containing evaluation metrics.\n    \"\"\"\n    weighted_accuracy = 0.0\n    results = []\n    for response in output.responses:\n        result = self.evaluate_sample(instance, response.response)\n        weighted_accuracy += result.score * response.weight\n        results.append(\n            {\n                \"score\": result.score,\n                \"desc\": result.desc,\n                \"metadata\": result.metadata,\n            }\n        )\n\n    return {\n        \"weighted_accuracy\": weighted_accuracy,\n        \"runtime_seconds\": output.runtime_seconds,\n        \"results\": results,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.EvaluationResult","title":"<code>EvaluationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for storing evaluation results.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class EvaluationResult(BaseModel):\n    \"\"\"Class for storing evaluation results.\"\"\"\n\n    score: float\n    desc: str\n    metadata: Dict[str, Any] = {}\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.ModelOutput","title":"<code>ModelOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of model responses with execution metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelOutput(BaseModel):\n    \"\"\"Collection of model responses with execution metadata.\"\"\"\n\n    responses: List[ModelResponse]\n    runtime_seconds: Optional[float] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.ModelResponse","title":"<code>ModelResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single model response containing generated text, probability, and optional metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelResponse(BaseModel):\n    \"\"\"Single model response containing generated text, probability, and optional metadata.\"\"\"\n\n    response: Any\n    weight: float\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/__init__/#genlm.eval.run_evaluation","title":"<code>run_evaluation(dataset, model, evaluator, output_dir=None, n_replicates=1, overwrite_results=False, overwrite_outputs=False, max_instances=float('inf'), verbosity=0)</code>  <code>async</code>","text":"<p>Run evaluation on a dataset using the provided model and evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to evaluate on.</p> required <code>model</code> <code>ModelAdaptor</code> <p>The model adaptor to use for generation.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The evaluator to use for prompt generation and evaluation.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the results. Defaults to None, in which case results are not saved.</p> <code>None</code> <code>n_replicates</code> <code>int</code> <p>Number of times to replicate the evaluation. Defaults to 1.</p> <code>1</code> <code>overwrite_results</code> <code>bool</code> <p>Whether to overwrite existing evaluation results. Defaults to False.</p> <code>False</code> <code>overwrite_outputs</code> <code>bool</code> <p>Whether to overwrite existing output. Defaults to False.</p> <code>False</code> <code>max_instances</code> <code>int</code> <p>The maximum number of instances to evaluate. Defaults to float(\"inf\").</p> <code>float('inf')</code> <code>verbosity</code> <code>int</code> <p>The verbosity of the evaluation. Defaults to 0, which is silent.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Aggregated evaluation results.</p> Source code in <code>genlm/eval/core/runner.py</code> <pre><code>async def run_evaluation(\n    dataset,\n    model,\n    evaluator,\n    output_dir=None,\n    n_replicates=1,\n    overwrite_results=False,\n    overwrite_outputs=False,\n    max_instances=float(\"inf\"),\n    verbosity=0,\n):\n    \"\"\"Run evaluation on a dataset using the provided model and evaluator.\n\n    Args:\n        dataset (Dataset): The dataset to evaluate on.\n        model (ModelAdaptor): The model adaptor to use for generation.\n        evaluator (Evaluator): The evaluator to use for prompt generation and evaluation.\n        output_dir (str, optional): The directory to save the results. Defaults to None, in which case results are not saved.\n        n_replicates (int, optional): Number of times to replicate the evaluation. Defaults to 1.\n        overwrite_results (bool, optional): Whether to overwrite existing evaluation results. Defaults to False.\n        overwrite_outputs (bool, optional): Whether to overwrite existing output. Defaults to False.\n        max_instances (int, optional): The maximum number of instances to evaluate. Defaults to float(\"inf\").\n        verbosity (int, optional): The verbosity of the evaluation. Defaults to 0, which is silent.\n\n    Returns:\n        (Dict[str, Any]): Aggregated evaluation results.\n    \"\"\"\n    all_results = []\n    all_instance_results = []\n    all_instance_outputs = []\n\n    if overwrite_outputs and not overwrite_results:\n        raise ValueError(\n            \"Cannot overwrite outputs without overwriting results. (Hint: set overwrite_results=True)\"\n        )\n\n    if output_dir is not None and not os.path.exists(output_dir):\n        os.makedirs(output_dir)  # pragma: no cover\n\n    n_instances = 0\n    for instance in dataset:\n        n_instances += 1\n\n        instance_results = []\n        instance_outputs = []\n        instance_id = instance.instance_id\n\n        for i in range(n_replicates):\n            output = None\n            result = None\n            if output_dir is not None:\n                instance_output_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-output.json\"\n                )\n                instance_results_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-results.json\"\n                )\n\n                # Try loading cached files if not overwriting\n                if not overwrite_outputs:\n                    output = _load_cached_output(instance_output_path)\n                if not overwrite_results:\n                    result = _load_cached_results(instance_results_path)\n            else:\n                instance_output_path = None\n                instance_results_path = None\n\n            # Generate new output if needed\n            wrote_output = False\n            if output is None:\n                output = await model(instance, output_dir, replicate=i)\n                if instance_output_path is not None:\n                    wrote_output = True\n                    _save_output(output, instance_output_path)\n\n            # Evaluate if we need new results (no results, overwriting results, or wrote new output)\n            if result is None or overwrite_results or wrote_output:\n                result = evaluator.evaluate_ensemble(instance, output)\n                if instance_results_path is not None:\n                    _save_results(result, instance_results_path)\n\n            instance_results.append(result)\n            instance_outputs.append(output)\n\n        avg_instance_result = {\n            \"weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in instance_results)\n            / n_replicates,\n        }\n        all_results.append(avg_instance_result)\n        all_instance_results.append(instance_results)\n        all_instance_outputs.append(instance_outputs)\n\n        if verbosity &gt; 0:\n            print(f\"Instance {instance}\")\n            print(\n                f\"Mean weighted accuracy (instance): {avg_instance_result['weighted_accuracy']}\"\n            )\n            print(\n                f\"Mean weighted accuracy (total): {sum(r['weighted_accuracy'] for r in all_results) / len(all_results)}\"\n            )\n            print()\n\n        if n_instances &gt;= max_instances:\n            break\n\n    return {\n        \"average_weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in all_results)\n        / len(all_results),\n        \"n_instances\": len(all_results),\n        \"all_instance_results\": all_instance_results,\n        \"all_instance_outputs\": all_instance_outputs,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/util/","title":"util","text":""},{"location":"reference/genlm/eval/util/#genlm.eval.util","title":"<code>genlm.eval.util</code>","text":""},{"location":"reference/genlm/eval/util/#genlm.eval.util.bootstrap_ci","title":"<code>bootstrap_ci(values, metric, ci=0.95, n_bootstrap=10000)</code>","text":"<p>Calculate bootstrap confidence intervals for a given metric.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>array - like</code> <p>Array-like object containing the data points.</p> required <code>metric</code> <code>function</code> <p>Function that computes the statistic of interest (e.g., np.mean, np.median).</p> required <code>ci</code> <code>float</code> <p>Confidence interval level (between 0 and 1). Default is 0.95 for 95% CI.</p> <code>0.95</code> <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples to generate. Default is 10000.</p> <code>10000</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(mean, lower, upper) where: - mean is the metric computed on the original data - lower is the lower bound of the confidence interval - upper is the upper bound of the confidence interval</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If ci is not between 0 and 1 or if values is empty.</p> Source code in <code>genlm/eval/util.py</code> <pre><code>def bootstrap_ci(values, metric, ci=0.95, n_bootstrap=10000):\n    \"\"\"Calculate bootstrap confidence intervals for a given metric.\n\n    Args:\n        values (array-like): Array-like object containing the data points.\n        metric (function): Function that computes the statistic of interest (e.g., np.mean, np.median).\n        ci (float): Confidence interval level (between 0 and 1). Default is 0.95 for 95% CI.\n        n_bootstrap (int): Number of bootstrap samples to generate. Default is 10000.\n\n    Returns:\n        (tuple): (mean, lower, upper) where:\n            - mean is the metric computed on the original data\n            - lower is the lower bound of the confidence interval\n            - upper is the upper bound of the confidence interval\n\n    Raises:\n        ValueError: If ci is not between 0 and 1 or if values is empty.\n    \"\"\"\n    if not 0 &lt; ci &lt; 1:\n        raise ValueError(\"ci must be between 0 and 1\")\n\n    if not values:\n        raise ValueError(\"values must not be empty\")\n\n    values = np.asarray(values)\n\n    mean = metric(values)\n    bootstrap_values = []\n\n    for _ in range(n_bootstrap):\n        bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n        bootstrap_values.append(metric(bootstrap_sample))\n\n    lower_percentile = (1 - ci) / 2 * 100\n    upper_percentile = (1 + ci) / 2 * 100\n    lower, upper = np.percentile(bootstrap_values, [lower_percentile, upper_percentile])\n    return mean, lower, upper\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.chat_template_messages","title":"<code>chat_template_messages(prompt, examples, user_message)</code>","text":"<p>Create a list of messages for chat template formatting.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>System prompt to be used as the initial message.</p> required <code>examples</code> <code>list</code> <p>List of (example, response) tuples for few-shot learning.</p> required <code>user_message</code> <code>str</code> <p>The actual user query to be processed.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of dictionaries containing the formatted messages with roles  and content for the chat template.</p> Source code in <code>genlm/eval/util.py</code> <pre><code>def chat_template_messages(prompt, examples, user_message):\n    \"\"\"Create a list of messages for chat template formatting.\n\n    Args:\n        prompt (str): System prompt to be used as the initial message.\n        examples (list): List of (example, response) tuples for few-shot learning.\n        user_message (str): The actual user query to be processed.\n\n    Returns:\n        (list): List of dictionaries containing the formatted messages with roles\n             and content for the chat template.\n    \"\"\"\n    messages = [{\"role\": \"system\", \"content\": prompt}]\n    for example, response in examples:\n        messages.append({\"role\": \"user\", \"content\": example})\n        messages.append({\"role\": \"assistant\", \"content\": response})\n    messages.append({\"role\": \"user\", \"content\": user_message})\n    return messages\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.LRUCache","title":"<code>LRUCache</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A cache that evicts the least recently used item when the cache is full.</p> Source code in <code>genlm/eval/util.py</code> <pre><code>class LRUCache(ABC):\n    \"\"\"A cache that evicts the least recently used item when the cache is full.\"\"\"\n\n    def __init__(self, cache_size=1):\n        \"\"\"Initialize the cache.\n\n        Args:\n            cache_size (int): The size of the cache.\n        \"\"\"\n        self.cache_size = cache_size\n        self.cache = OrderedDict()\n\n    @abstractmethod\n    def create(self, key):\n        \"\"\"Create an object for a given key.\n\n        Args:\n            key (any): The key to create an object for.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def cleanup(self, key, obj):\n        \"\"\"Cleanup an object for a given key.\n\n        Args:\n            key (any): The key to cleanup an object for.\n            obj (any): The object to cleanup.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def get(self, key):\n        \"\"\"Get an object for a given key.\n\n        Args:\n            key (any): The key to get an object for.\n\n        Returns:\n            any: The object for the given key.\n        \"\"\"\n        if self.cache_size &gt; 0:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n                return self.cache[key]\n        else:\n            return self.create(key)\n\n        obj = self.create(key)\n        self.cache[key] = obj\n\n        if len(self.cache) &gt; self.cache_size:\n            key, obj = self.cache.popitem(last=False)\n            self.cleanup(key, obj)\n\n        return obj\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.LRUCache.__init__","title":"<code>__init__(cache_size=1)</code>","text":"<p>Initialize the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache_size</code> <code>int</code> <p>The size of the cache.</p> <code>1</code> Source code in <code>genlm/eval/util.py</code> <pre><code>def __init__(self, cache_size=1):\n    \"\"\"Initialize the cache.\n\n    Args:\n        cache_size (int): The size of the cache.\n    \"\"\"\n    self.cache_size = cache_size\n    self.cache = OrderedDict()\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.LRUCache.create","title":"<code>create(key)</code>  <code>abstractmethod</code>","text":"<p>Create an object for a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>any</code> <p>The key to create an object for.</p> required Source code in <code>genlm/eval/util.py</code> <pre><code>@abstractmethod\ndef create(self, key):\n    \"\"\"Create an object for a given key.\n\n    Args:\n        key (any): The key to create an object for.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.LRUCache.cleanup","title":"<code>cleanup(key, obj)</code>","text":"<p>Cleanup an object for a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>any</code> <p>The key to cleanup an object for.</p> required <code>obj</code> <code>any</code> <p>The object to cleanup.</p> required Source code in <code>genlm/eval/util.py</code> <pre><code>def cleanup(self, key, obj):\n    \"\"\"Cleanup an object for a given key.\n\n    Args:\n        key (any): The key to cleanup an object for.\n        obj (any): The object to cleanup.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/util/#genlm.eval.util.LRUCache.get","title":"<code>get(key)</code>","text":"<p>Get an object for a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>any</code> <p>The key to get an object for.</p> required <p>Returns:</p> Name Type Description <code>any</code> <p>The object for the given key.</p> Source code in <code>genlm/eval/util.py</code> <pre><code>def get(self, key):\n    \"\"\"Get an object for a given key.\n\n    Args:\n        key (any): The key to get an object for.\n\n    Returns:\n        any: The object for the given key.\n    \"\"\"\n    if self.cache_size &gt; 0:\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n    else:\n        return self.create(key)\n\n    obj = self.create(key)\n    self.cache[key] = obj\n\n    if len(self.cache) &gt; self.cache_size:\n        key, obj = self.cache.popitem(last=False)\n        self.cleanup(key, obj)\n\n    return obj\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/","title":"core","text":""},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core","title":"<code>genlm.eval.core</code>","text":""},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for datasets that yield instances conforming to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Dataset(Generic[T], ABC):\n    \"\"\"Base class for datasets that yield instances conforming to a Pydantic schema.\"\"\"\n\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[T]:\n        \"\"\"Iterate over dataset instances.\n\n        Returns:\n            Iterator[T]: An iterator over instances conforming to schema T.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def schema(self) -&gt; type[T]:\n        \"\"\"Get the Pydantic schema class for this dataset.\n\n        Returns:\n            type[T]: The Pydantic model class defining the schema.\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over dataset instances.</p> <p>Returns:</p> Type Description <code>Iterator[T]</code> <p>Iterator[T]: An iterator over instances conforming to schema T.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Iterator[T]:\n    \"\"\"Iterate over dataset instances.\n\n    Returns:\n        Iterator[T]: An iterator over instances conforming to schema T.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Dataset.schema","title":"<code>schema</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the Pydantic schema class for this dataset.</p> <p>Returns:</p> Type Description <code>type[T]</code> <p>type[T]: The Pydantic model class defining the schema.</p>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Instance","title":"<code>Instance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for dataset instances that conform to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Instance(BaseModel):\n    \"\"\"Base class for dataset instances that conform to a Pydantic schema.\"\"\"\n\n    instance_id: Union[int, str]\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.ModelAdaptor","title":"<code>ModelAdaptor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for async model adapters that process instances into model outputs.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelAdaptor(Protocol):\n    \"\"\"Protocol for async model adapters that process instances into model outputs.\"\"\"\n\n    async def __call__(\n        self, instance: Instance, output_dir: str, replicate: int\n    ) -&gt; ModelOutput:\n        \"\"\"Process an instance and generate model outputs.\n\n        Args:\n            instance (Instance): Input dataset instance to process\n            output_dir (str): Directory for saving any intermediate results\n            replicate (int): Replicate index for multiple evaluation runs\n\n        Returns:\n            (ModelOutput): Model output containing responses and runtime information\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.ModelAdaptor.__call__","title":"<code>__call__(instance, output_dir, replicate)</code>  <code>async</code>","text":"<p>Process an instance and generate model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance</code> <p>Input dataset instance to process</p> required <code>output_dir</code> <code>str</code> <p>Directory for saving any intermediate results</p> required <code>replicate</code> <code>int</code> <p>Replicate index for multiple evaluation runs</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>Model output containing responses and runtime information</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>async def __call__(\n    self, instance: Instance, output_dir: str, replicate: int\n) -&gt; ModelOutput:\n    \"\"\"Process an instance and generate model outputs.\n\n    Args:\n        instance (Instance): Input dataset instance to process\n        output_dir (str): Directory for saving any intermediate results\n        replicate (int): Replicate index for multiple evaluation runs\n\n    Returns:\n        (ModelOutput): Model output containing responses and runtime information\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.ModelOutput","title":"<code>ModelOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of model responses with execution metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelOutput(BaseModel):\n    \"\"\"Collection of model responses with execution metadata.\"\"\"\n\n    responses: List[ModelResponse]\n    runtime_seconds: Optional[float] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.ModelResponse","title":"<code>ModelResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single model response containing generated text, probability, and optional metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelResponse(BaseModel):\n    \"\"\"Single model response containing generated text, probability, and optional metadata.\"\"\"\n\n    response: Any\n    weight: float\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Evaluator","title":"<code>Evaluator</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for evaluators that handle response evaluation.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class Evaluator(Generic[T], ABC):\n    \"\"\"Base class for evaluators that handle response evaluation.\"\"\"\n\n    @abstractmethod\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate a single response for correctness.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n        Returns:\n            (EvaluationResult): The evaluation result.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            output (ModelOutput): The complete model output including ensemble responses.\n\n        Returns:\n            (Dict[str, Any]): Dictionary containing evaluation metrics.\n        \"\"\"\n        weighted_accuracy = 0.0\n        results = []\n        for response in output.responses:\n            result = self.evaluate_sample(instance, response.response)\n            weighted_accuracy += result.score * response.weight\n            results.append(\n                {\n                    \"score\": result.score,\n                    \"desc\": result.desc,\n                    \"metadata\": result.metadata,\n                }\n            )\n\n        return {\n            \"weighted_accuracy\": weighted_accuracy,\n            \"runtime_seconds\": output.runtime_seconds,\n            \"results\": results,\n        }\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Evaluator.evaluate_sample","title":"<code>evaluate_sample(instance, response)</code>  <code>abstractmethod</code>","text":"<p>Evaluate a single response for correctness.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>response</code> <code>Any</code> <p>The model's response, which is given by the response attribute of a <code>ModelOutput</code> object.</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>The evaluation result.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>@abstractmethod\ndef evaluate_sample(self, instance, response):\n    \"\"\"Evaluate a single response for correctness.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n    Returns:\n        (EvaluationResult): The evaluation result.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.Evaluator.evaluate_ensemble","title":"<code>evaluate_ensemble(instance, output)</code>","text":"<p>Evaluate the complete ensemble of weighted samples using weighted accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>output</code> <code>ModelOutput</code> <p>The complete model output including ensemble responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing evaluation metrics.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        output (ModelOutput): The complete model output including ensemble responses.\n\n    Returns:\n        (Dict[str, Any]): Dictionary containing evaluation metrics.\n    \"\"\"\n    weighted_accuracy = 0.0\n    results = []\n    for response in output.responses:\n        result = self.evaluate_sample(instance, response.response)\n        weighted_accuracy += result.score * response.weight\n        results.append(\n            {\n                \"score\": result.score,\n                \"desc\": result.desc,\n                \"metadata\": result.metadata,\n            }\n        )\n\n    return {\n        \"weighted_accuracy\": weighted_accuracy,\n        \"runtime_seconds\": output.runtime_seconds,\n        \"results\": results,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.EvaluationResult","title":"<code>EvaluationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for storing evaluation results.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class EvaluationResult(BaseModel):\n    \"\"\"Class for storing evaluation results.\"\"\"\n\n    score: float\n    desc: str\n    metadata: Dict[str, Any] = {}\n</code></pre>"},{"location":"reference/genlm/eval/core/__init__/#genlm.eval.core.run_evaluation","title":"<code>run_evaluation(dataset, model, evaluator, output_dir=None, n_replicates=1, overwrite_results=False, overwrite_outputs=False, max_instances=float('inf'), verbosity=0)</code>  <code>async</code>","text":"<p>Run evaluation on a dataset using the provided model and evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to evaluate on.</p> required <code>model</code> <code>ModelAdaptor</code> <p>The model adaptor to use for generation.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The evaluator to use for prompt generation and evaluation.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the results. Defaults to None, in which case results are not saved.</p> <code>None</code> <code>n_replicates</code> <code>int</code> <p>Number of times to replicate the evaluation. Defaults to 1.</p> <code>1</code> <code>overwrite_results</code> <code>bool</code> <p>Whether to overwrite existing evaluation results. Defaults to False.</p> <code>False</code> <code>overwrite_outputs</code> <code>bool</code> <p>Whether to overwrite existing output. Defaults to False.</p> <code>False</code> <code>max_instances</code> <code>int</code> <p>The maximum number of instances to evaluate. Defaults to float(\"inf\").</p> <code>float('inf')</code> <code>verbosity</code> <code>int</code> <p>The verbosity of the evaluation. Defaults to 0, which is silent.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Aggregated evaluation results.</p> Source code in <code>genlm/eval/core/runner.py</code> <pre><code>async def run_evaluation(\n    dataset,\n    model,\n    evaluator,\n    output_dir=None,\n    n_replicates=1,\n    overwrite_results=False,\n    overwrite_outputs=False,\n    max_instances=float(\"inf\"),\n    verbosity=0,\n):\n    \"\"\"Run evaluation on a dataset using the provided model and evaluator.\n\n    Args:\n        dataset (Dataset): The dataset to evaluate on.\n        model (ModelAdaptor): The model adaptor to use for generation.\n        evaluator (Evaluator): The evaluator to use for prompt generation and evaluation.\n        output_dir (str, optional): The directory to save the results. Defaults to None, in which case results are not saved.\n        n_replicates (int, optional): Number of times to replicate the evaluation. Defaults to 1.\n        overwrite_results (bool, optional): Whether to overwrite existing evaluation results. Defaults to False.\n        overwrite_outputs (bool, optional): Whether to overwrite existing output. Defaults to False.\n        max_instances (int, optional): The maximum number of instances to evaluate. Defaults to float(\"inf\").\n        verbosity (int, optional): The verbosity of the evaluation. Defaults to 0, which is silent.\n\n    Returns:\n        (Dict[str, Any]): Aggregated evaluation results.\n    \"\"\"\n    all_results = []\n    all_instance_results = []\n    all_instance_outputs = []\n\n    if overwrite_outputs and not overwrite_results:\n        raise ValueError(\n            \"Cannot overwrite outputs without overwriting results. (Hint: set overwrite_results=True)\"\n        )\n\n    if output_dir is not None and not os.path.exists(output_dir):\n        os.makedirs(output_dir)  # pragma: no cover\n\n    n_instances = 0\n    for instance in dataset:\n        n_instances += 1\n\n        instance_results = []\n        instance_outputs = []\n        instance_id = instance.instance_id\n\n        for i in range(n_replicates):\n            output = None\n            result = None\n            if output_dir is not None:\n                instance_output_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-output.json\"\n                )\n                instance_results_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-results.json\"\n                )\n\n                # Try loading cached files if not overwriting\n                if not overwrite_outputs:\n                    output = _load_cached_output(instance_output_path)\n                if not overwrite_results:\n                    result = _load_cached_results(instance_results_path)\n            else:\n                instance_output_path = None\n                instance_results_path = None\n\n            # Generate new output if needed\n            wrote_output = False\n            if output is None:\n                output = await model(instance, output_dir, replicate=i)\n                if instance_output_path is not None:\n                    wrote_output = True\n                    _save_output(output, instance_output_path)\n\n            # Evaluate if we need new results (no results, overwriting results, or wrote new output)\n            if result is None or overwrite_results or wrote_output:\n                result = evaluator.evaluate_ensemble(instance, output)\n                if instance_results_path is not None:\n                    _save_results(result, instance_results_path)\n\n            instance_results.append(result)\n            instance_outputs.append(output)\n\n        avg_instance_result = {\n            \"weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in instance_results)\n            / n_replicates,\n        }\n        all_results.append(avg_instance_result)\n        all_instance_results.append(instance_results)\n        all_instance_outputs.append(instance_outputs)\n\n        if verbosity &gt; 0:\n            print(f\"Instance {instance}\")\n            print(\n                f\"Mean weighted accuracy (instance): {avg_instance_result['weighted_accuracy']}\"\n            )\n            print(\n                f\"Mean weighted accuracy (total): {sum(r['weighted_accuracy'] for r in all_results) / len(all_results)}\"\n            )\n            print()\n\n        if n_instances &gt;= max_instances:\n            break\n\n    return {\n        \"average_weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in all_results)\n        / len(all_results),\n        \"n_instances\": len(all_results),\n        \"all_instance_results\": all_instance_results,\n        \"all_instance_outputs\": all_instance_outputs,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/core/dataset/","title":"dataset","text":""},{"location":"reference/genlm/eval/core/dataset/#genlm.eval.core.dataset","title":"<code>genlm.eval.core.dataset</code>","text":""},{"location":"reference/genlm/eval/core/dataset/#genlm.eval.core.dataset.Instance","title":"<code>Instance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for dataset instances that conform to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Instance(BaseModel):\n    \"\"\"Base class for dataset instances that conform to a Pydantic schema.\"\"\"\n\n    instance_id: Union[int, str]\n</code></pre>"},{"location":"reference/genlm/eval/core/dataset/#genlm.eval.core.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for datasets that yield instances conforming to a Pydantic schema.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>class Dataset(Generic[T], ABC):\n    \"\"\"Base class for datasets that yield instances conforming to a Pydantic schema.\"\"\"\n\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[T]:\n        \"\"\"Iterate over dataset instances.\n\n        Returns:\n            Iterator[T]: An iterator over instances conforming to schema T.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def schema(self) -&gt; type[T]:\n        \"\"\"Get the Pydantic schema class for this dataset.\n\n        Returns:\n            type[T]: The Pydantic model class defining the schema.\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/dataset/#genlm.eval.core.dataset.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over dataset instances.</p> <p>Returns:</p> Type Description <code>Iterator[T]</code> <p>Iterator[T]: An iterator over instances conforming to schema T.</p> Source code in <code>genlm/eval/core/dataset.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Iterator[T]:\n    \"\"\"Iterate over dataset instances.\n\n    Returns:\n        Iterator[T]: An iterator over instances conforming to schema T.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/dataset/#genlm.eval.core.dataset.Dataset.schema","title":"<code>schema</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the Pydantic schema class for this dataset.</p> <p>Returns:</p> Type Description <code>type[T]</code> <p>type[T]: The Pydantic model class defining the schema.</p>"},{"location":"reference/genlm/eval/core/evaluator/","title":"evaluator","text":""},{"location":"reference/genlm/eval/core/evaluator/#genlm.eval.core.evaluator","title":"<code>genlm.eval.core.evaluator</code>","text":""},{"location":"reference/genlm/eval/core/evaluator/#genlm.eval.core.evaluator.EvaluationResult","title":"<code>EvaluationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for storing evaluation results.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class EvaluationResult(BaseModel):\n    \"\"\"Class for storing evaluation results.\"\"\"\n\n    score: float\n    desc: str\n    metadata: Dict[str, Any] = {}\n</code></pre>"},{"location":"reference/genlm/eval/core/evaluator/#genlm.eval.core.evaluator.Evaluator","title":"<code>Evaluator</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Base class for evaluators that handle response evaluation.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>class Evaluator(Generic[T], ABC):\n    \"\"\"Base class for evaluators that handle response evaluation.\"\"\"\n\n    @abstractmethod\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate a single response for correctness.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n        Returns:\n            (EvaluationResult): The evaluation result.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n        Args:\n            instance (T): The dataset instance being evaluated.\n            output (ModelOutput): The complete model output including ensemble responses.\n\n        Returns:\n            (Dict[str, Any]): Dictionary containing evaluation metrics.\n        \"\"\"\n        weighted_accuracy = 0.0\n        results = []\n        for response in output.responses:\n            result = self.evaluate_sample(instance, response.response)\n            weighted_accuracy += result.score * response.weight\n            results.append(\n                {\n                    \"score\": result.score,\n                    \"desc\": result.desc,\n                    \"metadata\": result.metadata,\n                }\n            )\n\n        return {\n            \"weighted_accuracy\": weighted_accuracy,\n            \"runtime_seconds\": output.runtime_seconds,\n            \"results\": results,\n        }\n</code></pre>"},{"location":"reference/genlm/eval/core/evaluator/#genlm.eval.core.evaluator.Evaluator.evaluate_sample","title":"<code>evaluate_sample(instance, response)</code>  <code>abstractmethod</code>","text":"<p>Evaluate a single response for correctness.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>response</code> <code>Any</code> <p>The model's response, which is given by the response attribute of a <code>ModelOutput</code> object.</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>The evaluation result.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>@abstractmethod\ndef evaluate_sample(self, instance, response):\n    \"\"\"Evaluate a single response for correctness.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        response (Any): The model's response, which is given by the response attribute of a `ModelOutput` object.\n\n    Returns:\n        (EvaluationResult): The evaluation result.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/evaluator/#genlm.eval.core.evaluator.Evaluator.evaluate_ensemble","title":"<code>evaluate_ensemble(instance, output)</code>","text":"<p>Evaluate the complete ensemble of weighted samples using weighted accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>T</code> <p>The dataset instance being evaluated.</p> required <code>output</code> <code>ModelOutput</code> <p>The complete model output including ensemble responses.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing evaluation metrics.</p> Source code in <code>genlm/eval/core/evaluator.py</code> <pre><code>def evaluate_ensemble(self, instance: T, output: ModelOutput) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate the complete ensemble of weighted samples using weighted accuracy.\n\n    Args:\n        instance (T): The dataset instance being evaluated.\n        output (ModelOutput): The complete model output including ensemble responses.\n\n    Returns:\n        (Dict[str, Any]): Dictionary containing evaluation metrics.\n    \"\"\"\n    weighted_accuracy = 0.0\n    results = []\n    for response in output.responses:\n        result = self.evaluate_sample(instance, response.response)\n        weighted_accuracy += result.score * response.weight\n        results.append(\n            {\n                \"score\": result.score,\n                \"desc\": result.desc,\n                \"metadata\": result.metadata,\n            }\n        )\n\n    return {\n        \"weighted_accuracy\": weighted_accuracy,\n        \"runtime_seconds\": output.runtime_seconds,\n        \"results\": results,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/core/model/","title":"model","text":""},{"location":"reference/genlm/eval/core/model/#genlm.eval.core.model","title":"<code>genlm.eval.core.model</code>","text":""},{"location":"reference/genlm/eval/core/model/#genlm.eval.core.model.ModelResponse","title":"<code>ModelResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single model response containing generated text, probability, and optional metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelResponse(BaseModel):\n    \"\"\"Single model response containing generated text, probability, and optional metadata.\"\"\"\n\n    response: Any\n    weight: float\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/core/model/#genlm.eval.core.model.ModelOutput","title":"<code>ModelOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of model responses with execution metadata.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelOutput(BaseModel):\n    \"\"\"Collection of model responses with execution metadata.\"\"\"\n\n    responses: List[ModelResponse]\n    runtime_seconds: Optional[float] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genlm/eval/core/model/#genlm.eval.core.model.ModelAdaptor","title":"<code>ModelAdaptor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for async model adapters that process instances into model outputs.</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>class ModelAdaptor(Protocol):\n    \"\"\"Protocol for async model adapters that process instances into model outputs.\"\"\"\n\n    async def __call__(\n        self, instance: Instance, output_dir: str, replicate: int\n    ) -&gt; ModelOutput:\n        \"\"\"Process an instance and generate model outputs.\n\n        Args:\n            instance (Instance): Input dataset instance to process\n            output_dir (str): Directory for saving any intermediate results\n            replicate (int): Replicate index for multiple evaluation runs\n\n        Returns:\n            (ModelOutput): Model output containing responses and runtime information\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/model/#genlm.eval.core.model.ModelAdaptor.__call__","title":"<code>__call__(instance, output_dir, replicate)</code>  <code>async</code>","text":"<p>Process an instance and generate model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance</code> <p>Input dataset instance to process</p> required <code>output_dir</code> <code>str</code> <p>Directory for saving any intermediate results</p> required <code>replicate</code> <code>int</code> <p>Replicate index for multiple evaluation runs</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>Model output containing responses and runtime information</p> Source code in <code>genlm/eval/core/model.py</code> <pre><code>async def __call__(\n    self, instance: Instance, output_dir: str, replicate: int\n) -&gt; ModelOutput:\n    \"\"\"Process an instance and generate model outputs.\n\n    Args:\n        instance (Instance): Input dataset instance to process\n        output_dir (str): Directory for saving any intermediate results\n        replicate (int): Replicate index for multiple evaluation runs\n\n    Returns:\n        (ModelOutput): Model output containing responses and runtime information\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/eval/core/runner/","title":"runner","text":""},{"location":"reference/genlm/eval/core/runner/#genlm.eval.core.runner","title":"<code>genlm.eval.core.runner</code>","text":""},{"location":"reference/genlm/eval/core/runner/#genlm.eval.core.runner.run_evaluation","title":"<code>run_evaluation(dataset, model, evaluator, output_dir=None, n_replicates=1, overwrite_results=False, overwrite_outputs=False, max_instances=float('inf'), verbosity=0)</code>  <code>async</code>","text":"<p>Run evaluation on a dataset using the provided model and evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to evaluate on.</p> required <code>model</code> <code>ModelAdaptor</code> <p>The model adaptor to use for generation.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The evaluator to use for prompt generation and evaluation.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the results. Defaults to None, in which case results are not saved.</p> <code>None</code> <code>n_replicates</code> <code>int</code> <p>Number of times to replicate the evaluation. Defaults to 1.</p> <code>1</code> <code>overwrite_results</code> <code>bool</code> <p>Whether to overwrite existing evaluation results. Defaults to False.</p> <code>False</code> <code>overwrite_outputs</code> <code>bool</code> <p>Whether to overwrite existing output. Defaults to False.</p> <code>False</code> <code>max_instances</code> <code>int</code> <p>The maximum number of instances to evaluate. Defaults to float(\"inf\").</p> <code>float('inf')</code> <code>verbosity</code> <code>int</code> <p>The verbosity of the evaluation. Defaults to 0, which is silent.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Aggregated evaluation results.</p> Source code in <code>genlm/eval/core/runner.py</code> <pre><code>async def run_evaluation(\n    dataset,\n    model,\n    evaluator,\n    output_dir=None,\n    n_replicates=1,\n    overwrite_results=False,\n    overwrite_outputs=False,\n    max_instances=float(\"inf\"),\n    verbosity=0,\n):\n    \"\"\"Run evaluation on a dataset using the provided model and evaluator.\n\n    Args:\n        dataset (Dataset): The dataset to evaluate on.\n        model (ModelAdaptor): The model adaptor to use for generation.\n        evaluator (Evaluator): The evaluator to use for prompt generation and evaluation.\n        output_dir (str, optional): The directory to save the results. Defaults to None, in which case results are not saved.\n        n_replicates (int, optional): Number of times to replicate the evaluation. Defaults to 1.\n        overwrite_results (bool, optional): Whether to overwrite existing evaluation results. Defaults to False.\n        overwrite_outputs (bool, optional): Whether to overwrite existing output. Defaults to False.\n        max_instances (int, optional): The maximum number of instances to evaluate. Defaults to float(\"inf\").\n        verbosity (int, optional): The verbosity of the evaluation. Defaults to 0, which is silent.\n\n    Returns:\n        (Dict[str, Any]): Aggregated evaluation results.\n    \"\"\"\n    all_results = []\n    all_instance_results = []\n    all_instance_outputs = []\n\n    if overwrite_outputs and not overwrite_results:\n        raise ValueError(\n            \"Cannot overwrite outputs without overwriting results. (Hint: set overwrite_results=True)\"\n        )\n\n    if output_dir is not None and not os.path.exists(output_dir):\n        os.makedirs(output_dir)  # pragma: no cover\n\n    n_instances = 0\n    for instance in dataset:\n        n_instances += 1\n\n        instance_results = []\n        instance_outputs = []\n        instance_id = instance.instance_id\n\n        for i in range(n_replicates):\n            output = None\n            result = None\n            if output_dir is not None:\n                instance_output_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-output.json\"\n                )\n                instance_results_path = os.path.join(\n                    output_dir, f\"{instance_id}-{i}-results.json\"\n                )\n\n                # Try loading cached files if not overwriting\n                if not overwrite_outputs:\n                    output = _load_cached_output(instance_output_path)\n                if not overwrite_results:\n                    result = _load_cached_results(instance_results_path)\n            else:\n                instance_output_path = None\n                instance_results_path = None\n\n            # Generate new output if needed\n            wrote_output = False\n            if output is None:\n                output = await model(instance, output_dir, replicate=i)\n                if instance_output_path is not None:\n                    wrote_output = True\n                    _save_output(output, instance_output_path)\n\n            # Evaluate if we need new results (no results, overwriting results, or wrote new output)\n            if result is None or overwrite_results or wrote_output:\n                result = evaluator.evaluate_ensemble(instance, output)\n                if instance_results_path is not None:\n                    _save_results(result, instance_results_path)\n\n            instance_results.append(result)\n            instance_outputs.append(output)\n\n        avg_instance_result = {\n            \"weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in instance_results)\n            / n_replicates,\n        }\n        all_results.append(avg_instance_result)\n        all_instance_results.append(instance_results)\n        all_instance_outputs.append(instance_outputs)\n\n        if verbosity &gt; 0:\n            print(f\"Instance {instance}\")\n            print(\n                f\"Mean weighted accuracy (instance): {avg_instance_result['weighted_accuracy']}\"\n            )\n            print(\n                f\"Mean weighted accuracy (total): {sum(r['weighted_accuracy'] for r in all_results) / len(all_results)}\"\n            )\n            print()\n\n        if n_instances &gt;= max_instances:\n            break\n\n    return {\n        \"average_weighted_accuracy\": sum(r[\"weighted_accuracy\"] for r in all_results)\n        / len(all_results),\n        \"n_instances\": len(all_results),\n        \"all_instance_results\": all_instance_results,\n        \"all_instance_outputs\": all_instance_outputs,\n    }\n</code></pre>"},{"location":"reference/genlm/eval/domains/__init__/","title":"domains","text":""},{"location":"reference/genlm/eval/domains/__init__/#genlm.eval.domains","title":"<code>genlm.eval.domains</code>","text":""},{"location":"reference/genlm/eval/domains/molecular_synthesis/","title":"molecular_synthesis","text":""},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis","title":"<code>genlm.eval.domains.molecular_synthesis</code>","text":""},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisInstance","title":"<code>MolecularSynthesisInstance</code>","text":"<p>               Bases: <code>Instance</code></p> <p>Schema for molecular synthesis instance.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>class MolecularSynthesisInstance(Instance):\n    \"\"\"Schema for molecular synthesis instance.\"\"\"\n\n    molecules: list[str]\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisDataset","title":"<code>MolecularSynthesisDataset</code>","text":"<p>               Bases: <code>Dataset[MolecularSynthesisInstance]</code></p> <p>Dataset for molecular synthesis evaluation.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>class MolecularSynthesisDataset(Dataset[MolecularSynthesisInstance]):\n    \"\"\"Dataset for molecular synthesis evaluation.\"\"\"\n\n    def __init__(self, prompt_molecules):\n        \"\"\"Initialize the dataset with a list of molecules.\n\n        Args:\n            prompt_molecules: List of lists of molecules which will be used to generate prompts.\n        \"\"\"\n        self.prompt_molecules = prompt_molecules\n\n    def __len__(self):\n        return len(self.prompt_molecules)\n\n    @classmethod\n    def from_smiles(cls, smiles_path, n_molecules=20, n_instances=100, seed=1234):\n        \"\"\"Load molecules from a SMILES file.\n\n        Args:\n            smiles_path (str): Path to the .smi file containing SMILES strings.\n            n_molecules (int): Number of molecules to sample.\n            n_instances (int): Number of instances to sample.\n            seed (int): Seed for the random number generator.\n\n        Returns:\n            MolecularSynthesisDataset: Dataset initialized with molecules from the SMILES.\n        \"\"\"\n        molecules = open(smiles_path).readlines()\n        prompt_molecules = []\n        random.seed(seed)\n        for _ in range(n_instances):\n            molecule_ids = random.sample(range(len(molecules)), n_molecules)\n            prompt_molecules.append([molecules[i] for i in molecule_ids])\n        return cls(prompt_molecules)\n\n    def __iter__(self):\n        \"\"\"Iterate over molecules.\n\n        Returns:\n            Iterator[MolecularSynthesisInstance]: Iterator over molecular synthesis instances.\n        \"\"\"\n        for i, molecules in enumerate(self.prompt_molecules):\n            yield MolecularSynthesisInstance(molecules=molecules, instance_id=i)\n\n    @property\n    def schema(self):\n        \"\"\"Get the schema class for this dataset.\n\n        Returns:\n            type[MolecularSynthesisInstance]: The Pydantic model class for molecular synthesis instances.\n        \"\"\"\n        return MolecularSynthesisInstance\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisDataset.__init__","title":"<code>__init__(prompt_molecules)</code>","text":"<p>Initialize the dataset with a list of molecules.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_molecules</code> <p>List of lists of molecules which will be used to generate prompts.</p> required Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>def __init__(self, prompt_molecules):\n    \"\"\"Initialize the dataset with a list of molecules.\n\n    Args:\n        prompt_molecules: List of lists of molecules which will be used to generate prompts.\n    \"\"\"\n    self.prompt_molecules = prompt_molecules\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisDataset.from_smiles","title":"<code>from_smiles(smiles_path, n_molecules=20, n_instances=100, seed=1234)</code>  <code>classmethod</code>","text":"<p>Load molecules from a SMILES file.</p> <p>Parameters:</p> Name Type Description Default <code>smiles_path</code> <code>str</code> <p>Path to the .smi file containing SMILES strings.</p> required <code>n_molecules</code> <code>int</code> <p>Number of molecules to sample.</p> <code>20</code> <code>n_instances</code> <code>int</code> <p>Number of instances to sample.</p> <code>100</code> <code>seed</code> <code>int</code> <p>Seed for the random number generator.</p> <code>1234</code> <p>Returns:</p> Name Type Description <code>MolecularSynthesisDataset</code> <p>Dataset initialized with molecules from the SMILES.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>@classmethod\ndef from_smiles(cls, smiles_path, n_molecules=20, n_instances=100, seed=1234):\n    \"\"\"Load molecules from a SMILES file.\n\n    Args:\n        smiles_path (str): Path to the .smi file containing SMILES strings.\n        n_molecules (int): Number of molecules to sample.\n        n_instances (int): Number of instances to sample.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        MolecularSynthesisDataset: Dataset initialized with molecules from the SMILES.\n    \"\"\"\n    molecules = open(smiles_path).readlines()\n    prompt_molecules = []\n    random.seed(seed)\n    for _ in range(n_instances):\n        molecule_ids = random.sample(range(len(molecules)), n_molecules)\n        prompt_molecules.append([molecules[i] for i in molecule_ids])\n    return cls(prompt_molecules)\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over molecules.</p> <p>Returns:</p> Type Description <p>Iterator[MolecularSynthesisInstance]: Iterator over molecular synthesis instances.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over molecules.\n\n    Returns:\n        Iterator[MolecularSynthesisInstance]: Iterator over molecular synthesis instances.\n    \"\"\"\n    for i, molecules in enumerate(self.prompt_molecules):\n        yield MolecularSynthesisInstance(molecules=molecules, instance_id=i)\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisDataset.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>Get the schema class for this dataset.</p> <p>Returns:</p> Type Description <p>type[MolecularSynthesisInstance]: The Pydantic model class for molecular synthesis instances.</p>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisEvaluator","title":"<code>MolecularSynthesisEvaluator</code>","text":"<p>               Bases: <code>Evaluator[MolecularSynthesisInstance]</code></p> <p>Evaluator for molecular synthesis.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>class MolecularSynthesisEvaluator(Evaluator[MolecularSynthesisInstance]):\n    \"\"\"Evaluator for molecular synthesis.\"\"\"\n\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate if a response matches the regex pattern.\n\n        Args:\n            instance (PatternMatchingInstance): The pattern matching instance being evaluated.\n            response (str): The model's response text.\n\n        Returns:\n            (bool): Whether the response matches the pattern.\n        \"\"\"\n        valid, acc = cached_eval(response.strip())\n        desc = \"valid\" if valid else \"invalid\"\n        return EvaluationResult(score=acc, desc=desc)\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.MolecularSynthesisEvaluator.evaluate_sample","title":"<code>evaluate_sample(instance, response)</code>","text":"<p>Evaluate if a response matches the regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>PatternMatchingInstance</code> <p>The pattern matching instance being evaluated.</p> required <code>response</code> <code>str</code> <p>The model's response text.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the response matches the pattern.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>def evaluate_sample(self, instance, response):\n    \"\"\"Evaluate if a response matches the regex pattern.\n\n    Args:\n        instance (PatternMatchingInstance): The pattern matching instance being evaluated.\n        response (str): The model's response text.\n\n    Returns:\n        (bool): Whether the response matches the pattern.\n    \"\"\"\n    valid, acc = cached_eval(response.strip())\n    desc = \"valid\" if valid else \"invalid\"\n    return EvaluationResult(score=acc, desc=desc)\n</code></pre>"},{"location":"reference/genlm/eval/domains/molecular_synthesis/#genlm.eval.domains.molecular_synthesis.default_prompt_formatter","title":"<code>default_prompt_formatter(tokenizer, instance, use_chat_format=False, system_prompt=SYSTEM_PROMPT)</code>","text":"<p>Default prompt formatter for molecular synthesis.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>instance</code> <code>MolecularSynthesisInstance</code> <p>The instance to format.</p> required <code>use_chat_format</code> <code>bool</code> <p>Whether to use chat format.</p> <code>False</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use.</p> <code>SYSTEM_PROMPT</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The prompt ids.</p> Source code in <code>genlm/eval/domains/molecular_synthesis.py</code> <pre><code>def default_prompt_formatter(\n    tokenizer,\n    instance,\n    use_chat_format=False,\n    system_prompt=SYSTEM_PROMPT,\n):\n    \"\"\"Default prompt formatter for molecular synthesis.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to use.\n        instance (MolecularSynthesisInstance): The instance to format.\n        use_chat_format (bool): Whether to use chat format.\n        system_prompt (str): The system prompt to use.\n\n    Returns:\n        (list[int]): The prompt ids.\n    \"\"\"\n    if use_chat_format:\n        raise NotImplementedError(\n            \"Chat format not implemented for molecular synthesis.\"\n        )\n    else:\n        prompt_ids = tokenizer.encode(\n            system_prompt\n            + \"\\n\"\n            + \"\\n\".join(\"Molecule: \" + x for x in instance.molecules)\n            + \"\\nMolecule:\"\n        )\n\n    return prompt_ids\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/","title":"pattern_matching","text":""},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching","title":"<code>genlm.eval.domains.pattern_matching</code>","text":""},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingInstance","title":"<code>PatternMatchingInstance</code>","text":"<p>               Bases: <code>Instance</code></p> <p>Schema for pattern matching instance.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>class PatternMatchingInstance(Instance):\n    \"\"\"Schema for pattern matching instance.\"\"\"\n\n    pattern: str\n    instance_id: int\n\n    def __repr__(self):\n        return f\"pattern: {self.pattern} (id: {self.instance_id})\"\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingDataset","title":"<code>PatternMatchingDataset</code>","text":"<p>               Bases: <code>Dataset[PatternMatchingInstance]</code></p> <p>Dataset for pattern matching evaluation.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>class PatternMatchingDataset(Dataset[PatternMatchingInstance]):\n    \"\"\"Dataset for pattern matching evaluation.\"\"\"\n\n    def __init__(self, patterns):\n        \"\"\"Initialize the dataset with a list of regex patterns.\n\n        Args:\n            patterns (list[str]): List of regex patterns to evaluate.\n        \"\"\"\n        self.patterns = patterns\n\n    def __len__(self):\n        return len(self.patterns)\n\n    @classmethod\n    def from_csv(cls, csv_path, pattern_column):\n        \"\"\"Load patterns from a CSV file.\n\n        Args:\n            csv_path (str): Path to the CSV file.\n            pattern_column (str): Name of the column containing regex patterns.\n\n        Returns:\n            (PatternMatchingDataset): Dataset initialized with patterns from the CSV.\n        \"\"\"\n        patterns = pd.read_csv(csv_path)[pattern_column].to_list()\n        return cls(patterns)\n\n    def __iter__(self):\n        \"\"\"Iterate over regex patterns.\n\n        Returns:\n            (Iterator[PatternMatchingInstance]): Iterator over regex instances.\n        \"\"\"\n        for pattern_id, pattern in enumerate(self.patterns):\n            yield PatternMatchingInstance(pattern=pattern, instance_id=pattern_id)\n\n    @property\n    def schema(self):\n        \"\"\"Get the schema class for this dataset.\n\n        Returns:\n            (type[PatternMatchingInstance]): The Pydantic model class for pattern matching instances.\n        \"\"\"\n        return PatternMatchingInstance\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingDataset.__init__","title":"<code>__init__(patterns)</code>","text":"<p>Initialize the dataset with a list of regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>list[str]</code> <p>List of regex patterns to evaluate.</p> required Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>def __init__(self, patterns):\n    \"\"\"Initialize the dataset with a list of regex patterns.\n\n    Args:\n        patterns (list[str]): List of regex patterns to evaluate.\n    \"\"\"\n    self.patterns = patterns\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingDataset.from_csv","title":"<code>from_csv(csv_path, pattern_column)</code>  <code>classmethod</code>","text":"<p>Load patterns from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the CSV file.</p> required <code>pattern_column</code> <code>str</code> <p>Name of the column containing regex patterns.</p> required <p>Returns:</p> Type Description <code>PatternMatchingDataset</code> <p>Dataset initialized with patterns from the CSV.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>@classmethod\ndef from_csv(cls, csv_path, pattern_column):\n    \"\"\"Load patterns from a CSV file.\n\n    Args:\n        csv_path (str): Path to the CSV file.\n        pattern_column (str): Name of the column containing regex patterns.\n\n    Returns:\n        (PatternMatchingDataset): Dataset initialized with patterns from the CSV.\n    \"\"\"\n    patterns = pd.read_csv(csv_path)[pattern_column].to_list()\n    return cls(patterns)\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over regex patterns.</p> <p>Returns:</p> Type Description <code>Iterator[PatternMatchingInstance]</code> <p>Iterator over regex instances.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over regex patterns.\n\n    Returns:\n        (Iterator[PatternMatchingInstance]): Iterator over regex instances.\n    \"\"\"\n    for pattern_id, pattern in enumerate(self.patterns):\n        yield PatternMatchingInstance(pattern=pattern, instance_id=pattern_id)\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingDataset.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>Get the schema class for this dataset.</p> <p>Returns:</p> Type Description <code>type[PatternMatchingInstance]</code> <p>The Pydantic model class for pattern matching instances.</p>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingEvaluator","title":"<code>PatternMatchingEvaluator</code>","text":"<p>               Bases: <code>Evaluator[PatternMatchingInstance]</code></p> <p>Evaluator for pattern matching.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>class PatternMatchingEvaluator(Evaluator[PatternMatchingInstance]):\n    \"\"\"Evaluator for pattern matching.\"\"\"\n\n    def evaluate_sample(self, instance, response):\n        \"\"\"Evaluate if a response matches the regex pattern.\n\n        Args:\n            instance (PatternMatchingInstance): The pattern matching instance being evaluated.\n            response (str): The model's response text.\n\n        Returns:\n            (EvaluationResult): Evaluation result for whether the response matches the pattern.\n        \"\"\"\n        is_valid = regex.compile(instance.pattern).fullmatch(response) is not None\n        return EvaluationResult(\n            score=int(is_valid), desc=\"valid\" if is_valid else \"invalid\"\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternMatchingEvaluator.evaluate_sample","title":"<code>evaluate_sample(instance, response)</code>","text":"<p>Evaluate if a response matches the regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>PatternMatchingInstance</code> <p>The pattern matching instance being evaluated.</p> required <code>response</code> <code>str</code> <p>The model's response text.</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Evaluation result for whether the response matches the pattern.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>def evaluate_sample(self, instance, response):\n    \"\"\"Evaluate if a response matches the regex pattern.\n\n    Args:\n        instance (PatternMatchingInstance): The pattern matching instance being evaluated.\n        response (str): The model's response text.\n\n    Returns:\n        (EvaluationResult): Evaluation result for whether the response matches the pattern.\n    \"\"\"\n    is_valid = regex.compile(instance.pattern).fullmatch(response) is not None\n    return EvaluationResult(\n        score=int(is_valid), desc=\"valid\" if is_valid else \"invalid\"\n    )\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.PatternPotential","title":"<code>PatternPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Potential function for regex pattern matching.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>class PatternPotential(Potential):\n    \"\"\"Potential function for regex pattern matching.\"\"\"\n\n    def __init__(self, pattern):\n        vocab = list(map(ord, string.printable))\n        super().__init__(vocab)\n        self.r = regex.compile(pattern)\n\n    async def complete(self, context):\n        text = \"\".join(map(chr, context))\n        match = self.r.fullmatch(text) is not None\n        return 0.0 if match else float(\"-inf\")\n\n    async def prefix(self, context):\n        text = \"\".join(map(chr, context))\n        m = self.r.match(text, partial=True)\n        match = m is not None and m.start() == 0 and m.end() == len(text)\n        return 0.0 if match else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/eval/domains/pattern_matching/#genlm.eval.domains.pattern_matching.default_prompt_formatter","title":"<code>default_prompt_formatter(tokenizer, instance, use_chat_format=False, system_prompt=SYSTEM_PROMPT, few_shot_examples=FEW_SHOT_EXAMPLES)</code>","text":"<p>Default prompt formatter for pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>instance</code> <code>PatternMatchingInstance</code> <p>The instance to format.</p> required <code>use_chat_format</code> <code>bool</code> <p>Whether to use chat format.</p> <code>False</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use.</p> <code>SYSTEM_PROMPT</code> <code>few_shot_examples</code> <code>list[tuple[str, str]]</code> <p>The few shot examples to use. Each example is a tuple of (pattern, response).</p> <code>FEW_SHOT_EXAMPLES</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The prompt ids.</p> Source code in <code>genlm/eval/domains/pattern_matching.py</code> <pre><code>def default_prompt_formatter(\n    tokenizer,\n    instance,\n    use_chat_format=False,\n    system_prompt=SYSTEM_PROMPT,\n    few_shot_examples=FEW_SHOT_EXAMPLES,\n):\n    \"\"\"Default prompt formatter for pattern matching.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to use.\n        instance (PatternMatchingInstance): The instance to format.\n        use_chat_format (bool): Whether to use chat format.\n        system_prompt (str): The system prompt to use.\n        few_shot_examples (list[tuple[str, str]]): The few shot examples to use. Each example is a tuple of (pattern, response).\n\n    Returns:\n        (list[int]): The prompt ids.\n    \"\"\"\n    if use_chat_format:\n        return tokenizer.apply_chat_template(\n            chat_template_messages(\n                system_prompt,\n                few_shot_examples,\n                instance.pattern,\n            ),\n            tokenize=True,\n            add_generation_prompt=True,\n        )\n    else:\n        return tokenizer.encode(\n            (\n                system_prompt\n                + \"\\n\"\n                + \"\\n\".join(\n                    f\"Pattern: {input}\\nOutput: {output}\"\n                    for input, output in few_shot_examples\n                )\n                + \"\\n\"\n                + instance.pattern\n            )\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/__init__/","title":"spider","text":""},{"location":"reference/genlm/eval/domains/spider/__init__/#genlm.eval.domains.spider","title":"<code>genlm.eval.domains.spider</code>","text":""},{"location":"reference/genlm/eval/domains/spider/__init__/#genlm.eval.domains.spider.SpiderDataset","title":"<code>SpiderDataset</code>","text":"<p>               Bases: <code>Dataset[SpiderInstance]</code></p> <p>Dataset for text to SQL evaluation.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderDataset(Dataset[SpiderInstance]):\n    \"\"\"Dataset for text to SQL evaluation.\"\"\"\n\n    def __init__(\n        self,\n        dev_data,\n        spider_schemas,\n        train_data,\n        grammars=None,\n        few_shot_example_ids=None,\n    ):\n        self.dev_data = dev_data\n        self.spider_schemas = spider_schemas\n\n        if few_shot_example_ids is None:\n            few_shot_example_ids = [10, 100, 1000]  # pragma: no cover\n\n        self.few_shot_examples = []\n        for example_id in few_shot_example_ids:\n            train_datum = train_data[example_id]\n            self.few_shot_examples.append(\n                (\n                    self.user_message_template(\n                        serialize_schema(self.spider_schemas[train_datum.schema_name]),\n                        train_datum.utterance,\n                    ),\n                    train_datum.query,\n                )\n            )\n\n        self.grammars = grammars if grammars else {}\n\n    @staticmethod\n    def user_message_template(schema_str, utterance):\n        return (\n            \"Here is a database schema:\\n\"\n            f\"{schema_str}\\n\"\n            \"Please write me a SQL statement that answers the following question:\\n\"\n            f\"{utterance}\\n\"\n            \"Remember, DO NOT provide any commentary or explanation of what the code does, just the SQL statement ending in a semicolon.\"\n        )\n\n    @classmethod\n    def from_spider_dir(cls, raw_spider_dir, grammar_json_path=None, **kwargs):\n        raw_spider_dir = Path(raw_spider_dir)\n        dev_data = load_spider_data(raw_spider_dir / \"dev.json\")\n        spider_schemas = load_schemas(\n            schemas_path=raw_spider_dir / \"tables.json\",\n            db_path=raw_spider_dir / \"database\",\n        )\n        train_data = load_spider_data(raw_spider_dir / \"train_spider.json\")\n\n        if grammar_json_path is None:\n            grammars = None\n        else:\n            with open(grammar_json_path, \"r\") as f:\n                grammars = json.load(f)\n\n        return cls(dev_data, spider_schemas, train_data, grammars, **kwargs)\n\n    def __iter__(self):\n        for instance_id, dev_datum in enumerate(self.dev_data):\n            schema_str = serialize_schema(self.spider_schemas[dev_datum.schema_name])\n            yield SpiderInstance(\n                schema_name=dev_datum.schema_name,\n                schema_str=schema_str,\n                lark_grammar=self.grammars.get(dev_datum.schema_name),\n                utterance=dev_datum.utterance,\n                gold=dev_datum.query,\n                instance_id=instance_id,\n                few_shot_examples=self.few_shot_examples,\n                tables=self.spider_schemas[dev_datum.schema_name].tables,\n                user_message=self.user_message_template(\n                    schema_str,\n                    dev_datum.utterance,\n                ),\n            )\n\n    @property\n    def schema(self):\n        return SpiderInstance\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/__init__/#genlm.eval.domains.spider.SpiderEvaluator","title":"<code>SpiderEvaluator</code>","text":"<p>               Bases: <code>Evaluator[SpiderInstance]</code></p> <p>Evaluator for Spider.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderEvaluator(Evaluator[SpiderInstance]):\n    \"\"\"Evaluator for Spider.\"\"\"\n\n    def __init__(\n        self,\n        raw_spider_dir,\n        evaluator_timeout=None,\n    ):\n        self.raw_spider_dir = Path(raw_spider_dir)\n        self.evaluator = BaseSpiderEvaluator(\n            self.raw_spider_dir, timeout=evaluator_timeout\n        )\n\n    @lru_cache\n    def cached_eval(self, x, y, db):\n        return self.evaluator.evaluate(x, y, db_name=db)\n\n    def evaluate_sample(self, instance, response):\n        is_correct, reason, level = self.cached_eval(\n            response, instance.gold, instance.schema_name\n        )\n        if reason is None:\n            reason = \"valid\"\n        return EvaluationResult(\n            score=float(is_correct), desc=reason, metadata={\"level\": level}\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/__init__/#genlm.eval.domains.spider.SpiderInstance","title":"<code>SpiderInstance</code>","text":"<p>               Bases: <code>Instance</code></p> <p>Schema for text to SQL instance.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderInstance(Instance):\n    \"\"\"Schema for text to SQL instance.\"\"\"\n\n    utterance: str\n    schema_name: str\n    gold: str\n    schema_str: str\n    lark_grammar: Union[str, None]\n    few_shot_examples: List[tuple]\n    tables: List\n    user_message: str\n\n    def __str__(self):\n        return f\"utterance: {self.utterance}, schema_name: {self.schema_name} (id: {self.instance_id})\"\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/__init__/#genlm.eval.domains.spider.default_prompt_formatter","title":"<code>default_prompt_formatter(tokenizer, instance, use_chat_format=True, system_prompt=SYSTEM_PROMPT)</code>","text":"<p>Default prompt formatter for pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>instance</code> <code>SpiderInstance</code> <p>The instance to format.</p> required <code>use_chat_format</code> <code>bool</code> <p>Whether to use chat format.</p> <code>True</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use.</p> <code>SYSTEM_PROMPT</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The prompt ids.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>def default_prompt_formatter(\n    tokenizer,\n    instance,\n    use_chat_format=True,\n    system_prompt=SYSTEM_PROMPT,\n):\n    \"\"\"Default prompt formatter for pattern matching.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to use.\n        instance (SpiderInstance): The instance to format.\n        use_chat_format (bool): Whether to use chat format.\n        system_prompt (str): The system prompt to use.\n\n    Returns:\n        (list[int]): The prompt ids.\n    \"\"\"\n    if use_chat_format:\n        return tokenizer.apply_chat_template(\n            conversation=chat_template_messages(\n                system_prompt,\n                instance.few_shot_examples,\n                instance.user_message,\n            ),\n            tokenize=True,\n            add_generation_prompt=True,\n        )\n    else:\n        return tokenizer.encode(\n            (\n                system_prompt\n                + \"\\n\\n\"\n                + \"\\n\\n\".join(\n                    f\"{input}\\nSQL query: {output}\"\n                    for input, output in instance.few_shot_examples\n                )\n                + \"\\n\\n\"\n                + instance.user_message\n                + \"\\nSQL query:\"\n            )\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider/","title":"spider","text":""},{"location":"reference/genlm/eval/domains/spider/spider/#genlm.eval.domains.spider.spider","title":"<code>genlm.eval.domains.spider.spider</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider/#genlm.eval.domains.spider.spider.SpiderInstance","title":"<code>SpiderInstance</code>","text":"<p>               Bases: <code>Instance</code></p> <p>Schema for text to SQL instance.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderInstance(Instance):\n    \"\"\"Schema for text to SQL instance.\"\"\"\n\n    utterance: str\n    schema_name: str\n    gold: str\n    schema_str: str\n    lark_grammar: Union[str, None]\n    few_shot_examples: List[tuple]\n    tables: List\n    user_message: str\n\n    def __str__(self):\n        return f\"utterance: {self.utterance}, schema_name: {self.schema_name} (id: {self.instance_id})\"\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider/#genlm.eval.domains.spider.spider.SpiderDataset","title":"<code>SpiderDataset</code>","text":"<p>               Bases: <code>Dataset[SpiderInstance]</code></p> <p>Dataset for text to SQL evaluation.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderDataset(Dataset[SpiderInstance]):\n    \"\"\"Dataset for text to SQL evaluation.\"\"\"\n\n    def __init__(\n        self,\n        dev_data,\n        spider_schemas,\n        train_data,\n        grammars=None,\n        few_shot_example_ids=None,\n    ):\n        self.dev_data = dev_data\n        self.spider_schemas = spider_schemas\n\n        if few_shot_example_ids is None:\n            few_shot_example_ids = [10, 100, 1000]  # pragma: no cover\n\n        self.few_shot_examples = []\n        for example_id in few_shot_example_ids:\n            train_datum = train_data[example_id]\n            self.few_shot_examples.append(\n                (\n                    self.user_message_template(\n                        serialize_schema(self.spider_schemas[train_datum.schema_name]),\n                        train_datum.utterance,\n                    ),\n                    train_datum.query,\n                )\n            )\n\n        self.grammars = grammars if grammars else {}\n\n    @staticmethod\n    def user_message_template(schema_str, utterance):\n        return (\n            \"Here is a database schema:\\n\"\n            f\"{schema_str}\\n\"\n            \"Please write me a SQL statement that answers the following question:\\n\"\n            f\"{utterance}\\n\"\n            \"Remember, DO NOT provide any commentary or explanation of what the code does, just the SQL statement ending in a semicolon.\"\n        )\n\n    @classmethod\n    def from_spider_dir(cls, raw_spider_dir, grammar_json_path=None, **kwargs):\n        raw_spider_dir = Path(raw_spider_dir)\n        dev_data = load_spider_data(raw_spider_dir / \"dev.json\")\n        spider_schemas = load_schemas(\n            schemas_path=raw_spider_dir / \"tables.json\",\n            db_path=raw_spider_dir / \"database\",\n        )\n        train_data = load_spider_data(raw_spider_dir / \"train_spider.json\")\n\n        if grammar_json_path is None:\n            grammars = None\n        else:\n            with open(grammar_json_path, \"r\") as f:\n                grammars = json.load(f)\n\n        return cls(dev_data, spider_schemas, train_data, grammars, **kwargs)\n\n    def __iter__(self):\n        for instance_id, dev_datum in enumerate(self.dev_data):\n            schema_str = serialize_schema(self.spider_schemas[dev_datum.schema_name])\n            yield SpiderInstance(\n                schema_name=dev_datum.schema_name,\n                schema_str=schema_str,\n                lark_grammar=self.grammars.get(dev_datum.schema_name),\n                utterance=dev_datum.utterance,\n                gold=dev_datum.query,\n                instance_id=instance_id,\n                few_shot_examples=self.few_shot_examples,\n                tables=self.spider_schemas[dev_datum.schema_name].tables,\n                user_message=self.user_message_template(\n                    schema_str,\n                    dev_datum.utterance,\n                ),\n            )\n\n    @property\n    def schema(self):\n        return SpiderInstance\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider/#genlm.eval.domains.spider.spider.SpiderEvaluator","title":"<code>SpiderEvaluator</code>","text":"<p>               Bases: <code>Evaluator[SpiderInstance]</code></p> <p>Evaluator for Spider.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>class SpiderEvaluator(Evaluator[SpiderInstance]):\n    \"\"\"Evaluator for Spider.\"\"\"\n\n    def __init__(\n        self,\n        raw_spider_dir,\n        evaluator_timeout=None,\n    ):\n        self.raw_spider_dir = Path(raw_spider_dir)\n        self.evaluator = BaseSpiderEvaluator(\n            self.raw_spider_dir, timeout=evaluator_timeout\n        )\n\n    @lru_cache\n    def cached_eval(self, x, y, db):\n        return self.evaluator.evaluate(x, y, db_name=db)\n\n    def evaluate_sample(self, instance, response):\n        is_correct, reason, level = self.cached_eval(\n            response, instance.gold, instance.schema_name\n        )\n        if reason is None:\n            reason = \"valid\"\n        return EvaluationResult(\n            score=float(is_correct), desc=reason, metadata={\"level\": level}\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider/#genlm.eval.domains.spider.spider.default_prompt_formatter","title":"<code>default_prompt_formatter(tokenizer, instance, use_chat_format=True, system_prompt=SYSTEM_PROMPT)</code>","text":"<p>Default prompt formatter for pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>instance</code> <code>SpiderInstance</code> <p>The instance to format.</p> required <code>use_chat_format</code> <code>bool</code> <p>Whether to use chat format.</p> <code>True</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use.</p> <code>SYSTEM_PROMPT</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The prompt ids.</p> Source code in <code>genlm/eval/domains/spider/spider.py</code> <pre><code>def default_prompt_formatter(\n    tokenizer,\n    instance,\n    use_chat_format=True,\n    system_prompt=SYSTEM_PROMPT,\n):\n    \"\"\"Default prompt formatter for pattern matching.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to use.\n        instance (SpiderInstance): The instance to format.\n        use_chat_format (bool): Whether to use chat format.\n        system_prompt (str): The system prompt to use.\n\n    Returns:\n        (list[int]): The prompt ids.\n    \"\"\"\n    if use_chat_format:\n        return tokenizer.apply_chat_template(\n            conversation=chat_template_messages(\n                system_prompt,\n                instance.few_shot_examples,\n                instance.user_message,\n            ),\n            tokenize=True,\n            add_generation_prompt=True,\n        )\n    else:\n        return tokenizer.encode(\n            (\n                system_prompt\n                + \"\\n\\n\"\n                + \"\\n\\n\".join(\n                    f\"{input}\\nSQL query: {output}\"\n                    for input, output in instance.few_shot_examples\n                )\n                + \"\\n\\n\"\n                + instance.user_message\n                + \"\\nSQL query:\"\n            )\n        )\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/table_column_potential/","title":"table_column_potential","text":""},{"location":"reference/genlm/eval/domains/spider/table_column_potential/#genlm.eval.domains.spider.table_column_potential","title":"<code>genlm.eval.domains.spider.table_column_potential</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/__init__/","title":"spider_eval","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/__init__/#genlm.eval.domains.spider.spider_eval","title":"<code>genlm.eval.domains.spider.spider_eval</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/content_encoder/","title":"content_encoder","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/content_encoder/#genlm.eval.domains.spider.spider_eval.content_encoder","title":"<code>genlm.eval.domains.spider.spider_eval.content_encoder</code>","text":"<p>Copyright (c) 2020, salesforce.com, inc. All rights reserved. SPDX-License-Identifier: BSD-3-Clause For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause Encode DB content.</p> <p>Adapted from: https://github.com/ElementAI/picard/blob/5ddd6cb9f74efca87d4604d5ddddc1b638459466/seq2seq/utils/bridge_content_encoder.py which is adapted from: https://github.com/salesforce/TabularSemanticParsing/blob/0b094d329a5f0b5c038a820117ba4b6e7b6215cc/src/common/content_encoder.py</p>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/dialogue/","title":"dialogue","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/dialogue/#genlm.eval.domains.spider.spider_eval.dialogue","title":"<code>genlm.eval.domains.spider.spider_eval.dialogue</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluation/","title":"evaluation","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluation/#genlm.eval.domains.spider.spider_eval.evaluation","title":"<code>genlm.eval.domains.spider.spider_eval.evaluation</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluation/#genlm.eval.domains.spider.spider_eval.evaluation.eval_exec_match_worker","title":"<code>eval_exec_match_worker(db, p_str, g_str, pred, gold, result_queue)</code>","text":"<p>Worker function to execute SQL queries and compare results. This function is intended to run in a separate process.</p> Source code in <code>genlm/eval/domains/spider/spider_eval/evaluation.py</code> <pre><code>def eval_exec_match_worker(db, p_str, g_str, pred, gold, result_queue):\n    \"\"\"\n    Worker function to execute SQL queries and compare results.\n    This function is intended to run in a separate process.\n    \"\"\"\n    conn = None\n    cursor = None\n    try:\n        conn = sqlite3.connect(db)\n        cursor = conn.cursor()\n\n        try:\n            cursor.execute(p_str)\n            p_res = cursor.fetchall()\n        except Exception:\n            result_queue.put(False)\n            return\n\n        cursor.execute(g_str)\n        q_res = cursor.fetchall()\n\n        def res_map(res, val_units):\n            rmap = {}\n            for idx, val_unit in enumerate(val_units):\n                key = (\n                    tuple(val_unit[1])\n                    if not val_unit[2]\n                    else (val_unit[0], tuple(val_unit[1]), tuple(val_unit[2]))\n                )\n                rmap[key] = [r[idx] for r in res]\n            return rmap\n\n        p_val_units = [unit[1] for unit in pred[\"select\"][1]]\n        q_val_units = [unit[1] for unit in gold[\"select\"][1]]\n        result_queue.put(res_map(p_res, p_val_units) == res_map(q_res, q_val_units))\n\n    except Exception:\n        result_queue.put(False)\n\n    finally:\n        if cursor:\n            cursor.close()\n        if conn:\n            conn.close()\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluation/#genlm.eval.domains.spider.spider_eval.evaluation.eval_exec_match","title":"<code>eval_exec_match(db, p_str, g_str, pred, gold, timeout=None)</code>","text":"<p>Execute the eval_exec_match function with a timeout using multiprocessing.</p> Source code in <code>genlm/eval/domains/spider/spider_eval/evaluation.py</code> <pre><code>def eval_exec_match(db, p_str, g_str, pred, gold, timeout=None):\n    \"\"\"\n    Execute the eval_exec_match function with a timeout using multiprocessing.\n    \"\"\"\n    result_queue = multiprocessing.Queue()\n    process = multiprocessing.Process(\n        target=eval_exec_match_worker, args=(db, p_str, g_str, pred, gold, result_queue)\n    )\n    process.start()\n    if timeout is None:\n        process.join()\n    else:\n        process.join(timeout)\n\n    if process.is_alive():\n        process.terminate()\n        process.join()\n        print(f\"Query execution timed out after {timeout} seconds.\")\n        return False\n\n    return result_queue.get()\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluator/","title":"evaluator","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluator/#genlm.eval.domains.spider.spider_eval.evaluator","title":"<code>genlm.eval.domains.spider.spider_eval.evaluator</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluator/#genlm.eval.domains.spider.spider_eval.evaluator.Evaluator","title":"<code>Evaluator</code>","text":"Source code in <code>genlm/eval/domains/spider/spider_eval/evaluator.py</code> <pre><code>class Evaluator:\n    def __init__(self, spider_dir: Path, timeout=None):\n        self.tables_path = spider_dir / \"tables.json\"\n        self.db_path = spider_dir / \"database\"\n        self.kmaps = build_foreign_key_map_from_json(self.tables_path)\n        # self.official_evaluator = E.Evaluator()  # the official Spider Evaluator\n        self.timeout = timeout\n\n    def evaluate(self, gold: str, pred: str, db_name: str):\n        \"\"\"Returns: bool, Optional[str]\n\n        On success (i.e., predicted execution result is the same as gold), returns `(True, None)`\n        On failure, returns `(False, reason)` where reason is one of the two cases:\n        * `invalid` if `pred` sql is not a well-formed sql statement that can be parsed by sqlite\n        * `mismatch` if `pred` is a well-formed sql but the execution result is different from that of the `gold`.\n        \"\"\"\n        db = self.db_path / db_name / (db_name + \".sqlite\")\n        schema = Schema(get_schema(db))\n\n        try:\n            g_sql = get_sql(schema, gold)\n            p_sql = get_sql(schema, pred)\n        except Exception:\n            # sql is ill-formed (can't be parsed by sqlite engine)\n            return False, \"invalid\", None\n\n        kmap = self.kmaps[db_name]\n\n        g_valid_col_units = build_valid_col_units(g_sql[\"from\"][\"table_units\"], schema)\n        g_sql = rebuild_sql_val(g_sql)\n        g_sql = rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n\n        p_valid_col_units = build_valid_col_units(p_sql[\"from\"][\"table_units\"], schema)\n        p_sql = rebuild_sql_val(p_sql)\n        p_sql = rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n\n        exec_match = eval_exec_match(db, pred, gold, p_sql, g_sql, timeout=self.timeout)\n        reason = None if exec_match else \"mismatch\"\n\n        difficulty_level = eval_hardness(g_sql)\n        return exec_match, reason, difficulty_level\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/evaluator/#genlm.eval.domains.spider.spider_eval.evaluator.Evaluator.evaluate","title":"<code>evaluate(gold, pred, db_name)</code>","text":"<p>Returns: bool, Optional[str]</p> <p>On success (i.e., predicted execution result is the same as gold), returns <code>(True, None)</code> On failure, returns <code>(False, reason)</code> where reason is one of the two cases: * <code>invalid</code> if <code>pred</code> sql is not a well-formed sql statement that can be parsed by sqlite * <code>mismatch</code> if <code>pred</code> is a well-formed sql but the execution result is different from that of the <code>gold</code>.</p> Source code in <code>genlm/eval/domains/spider/spider_eval/evaluator.py</code> <pre><code>def evaluate(self, gold: str, pred: str, db_name: str):\n    \"\"\"Returns: bool, Optional[str]\n\n    On success (i.e., predicted execution result is the same as gold), returns `(True, None)`\n    On failure, returns `(False, reason)` where reason is one of the two cases:\n    * `invalid` if `pred` sql is not a well-formed sql statement that can be parsed by sqlite\n    * `mismatch` if `pred` is a well-formed sql but the execution result is different from that of the `gold`.\n    \"\"\"\n    db = self.db_path / db_name / (db_name + \".sqlite\")\n    schema = Schema(get_schema(db))\n\n    try:\n        g_sql = get_sql(schema, gold)\n        p_sql = get_sql(schema, pred)\n    except Exception:\n        # sql is ill-formed (can't be parsed by sqlite engine)\n        return False, \"invalid\", None\n\n    kmap = self.kmaps[db_name]\n\n    g_valid_col_units = build_valid_col_units(g_sql[\"from\"][\"table_units\"], schema)\n    g_sql = rebuild_sql_val(g_sql)\n    g_sql = rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n\n    p_valid_col_units = build_valid_col_units(p_sql[\"from\"][\"table_units\"], schema)\n    p_sql = rebuild_sql_val(p_sql)\n    p_sql = rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n\n    exec_match = eval_exec_match(db, pred, gold, p_sql, g_sql, timeout=self.timeout)\n    reason = None if exec_match else \"mismatch\"\n\n    difficulty_level = eval_hardness(g_sql)\n    return exec_match, reason, difficulty_level\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/paths/","title":"paths","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/paths/#genlm.eval.domains.spider.spider_eval.paths","title":"<code>genlm.eval.domains.spider.spider_eval.paths</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/","title":"process_sql","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql","title":"<code>genlm.eval.domains.spider.spider_eval.process_sql</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.Schema","title":"<code>Schema</code>","text":"<p>Simple schema which maps table&amp;column to a unique identifier</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>class Schema:\n    \"\"\"\n    Simple schema which maps table&amp;column to a unique identifier\n    \"\"\"\n\n    def __init__(self, schema):\n        self._schema = schema\n        self._idMap = self._map(self._schema)\n\n    @property\n    def schema(self):\n        return self._schema\n\n    @property\n    def idMap(self):\n        return self._idMap\n\n    def _map(self, schema):\n        idMap = {\"*\": \"__all__\"}\n        id = 1\n        for key, vals in schema.items():\n            for val in vals:\n                idMap[key.lower() + \".\" + val.lower()] = (\n                    \"__\" + key.lower() + \".\" + val.lower() + \"__\"\n                )\n                id += 1\n\n        for key in schema:\n            idMap[key.lower()] = \"__\" + key.lower() + \"__\"\n            id += 1\n\n        return idMap\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.get_schema","title":"<code>get_schema(db)</code>","text":"<p>Get database's schema, which is a dict with table name as key and list of column names as value :param db: database path :return: schema dict</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def get_schema(db):\n    \"\"\"\n    Get database's schema, which is a dict with table name as key\n    and list of column names as value\n    :param db: database path\n    :return: schema dict\n    \"\"\"\n\n    schema = {}\n    conn = sqlite3.connect(db)\n    cursor = conn.cursor()\n\n    # fetch table names\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = [str(table[0].lower()) for table in cursor.fetchall()]\n\n    # fetch table info\n    for table in tables:\n        cursor.execute(\"PRAGMA table_info({})\".format(table))\n        schema[table] = [str(col[1].lower()) for col in cursor.fetchall()]\n\n    return schema\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.scan_alias","title":"<code>scan_alias(toks)</code>","text":"<p>Scan the index of 'as' and build the map for all alias</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def scan_alias(toks):\n    \"\"\"Scan the index of 'as' and build the map for all alias\"\"\"\n    as_idxs = [idx for idx, tok in enumerate(toks) if tok == \"as\"]\n    alias = {}\n    for idx in as_idxs:\n        alias[toks[idx + 1]] = toks[idx - 1]\n    return alias\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.parse_col","title":"<code>parse_col(toks, start_idx, tables_with_alias, schema, default_tables=None)</code>","text":"<p>:returns next idx, column id</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def parse_col(toks, start_idx, tables_with_alias, schema, default_tables=None):\n    \"\"\"\n    :returns next idx, column id\n    \"\"\"\n    tok = toks[start_idx]\n    if tok == \"*\":\n        return start_idx + 1, schema.idMap[tok]\n\n    if \".\" in tok:  # if token is a composite\n        alias, col = tok.split(\".\")\n        key = tables_with_alias[alias] + \".\" + col\n        return start_idx + 1, schema.idMap[key]\n\n    assert default_tables is not None and len(default_tables) &gt; 0, (\n        \"Default tables should not be None or empty\"\n    )\n\n    for alias in default_tables:\n        table = tables_with_alias[alias]\n        if tok in schema.schema[table]:\n            key = table + \".\" + tok\n            return start_idx + 1, schema.idMap[key]\n\n    assert False, \"Error col: {}\".format(tok)\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.parse_col_unit","title":"<code>parse_col_unit(toks, start_idx, tables_with_alias, schema, default_tables=None)</code>","text":"<p>:returns next idx, (agg_op id, col_id)</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def parse_col_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n    \"\"\"\n    :returns next idx, (agg_op id, col_id)\n    \"\"\"\n    idx = start_idx\n    len_ = len(toks)\n    isBlock = False\n    isDistinct = False\n    if toks[idx] == \"(\":\n        isBlock = True\n        idx += 1\n\n    if toks[idx] in AGG_OPS:\n        agg_id = AGG_OPS.index(toks[idx])\n        idx += 1\n        assert idx &lt; len_ and toks[idx] == \"(\"\n        idx += 1\n        if toks[idx] == \"distinct\":\n            idx += 1\n            isDistinct = True\n        idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n        assert idx &lt; len_ and toks[idx] == \")\"\n        idx += 1\n        return idx, (agg_id, col_id, isDistinct)\n\n    if toks[idx] == \"distinct\":\n        idx += 1\n        isDistinct = True\n    agg_id = AGG_OPS.index(\"none\")\n    idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n\n    if isBlock:\n        assert toks[idx] == \")\"\n        idx += 1  # skip ')'\n\n    return idx, (agg_id, col_id, isDistinct)\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.parse_table_unit","title":"<code>parse_table_unit(toks, start_idx, tables_with_alias, schema)</code>","text":"<p>:returns next idx, table id, table name</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def parse_table_unit(toks, start_idx, tables_with_alias, schema):\n    \"\"\"\n    :returns next idx, table id, table name\n    \"\"\"\n    idx = start_idx\n    len_ = len(toks)\n    key = tables_with_alias[toks[idx]]\n\n    if idx + 1 &lt; len_ and toks[idx + 1] == \"as\":\n        idx += 3\n    else:\n        idx += 1\n\n    return idx, schema.idMap[key], key\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/process_sql/#genlm.eval.domains.spider.spider_eval.process_sql.parse_from","title":"<code>parse_from(toks, start_idx, tables_with_alias, schema)</code>","text":"<p>Assume in the from clause, all table units are combined with join</p> Source code in <code>genlm/eval/domains/spider/spider_eval/process_sql.py</code> <pre><code>def parse_from(toks, start_idx, tables_with_alias, schema):\n    \"\"\"\n    Assume in the from clause, all table units are combined with join\n    \"\"\"\n    assert \"from\" in toks[start_idx:], \"'from' not found\"\n\n    len_ = len(toks)\n    idx = toks.index(\"from\", start_idx) + 1\n    default_tables = []\n    table_units = []\n    conds = []\n\n    while idx &lt; len_:\n        isBlock = False\n        if toks[idx] == \"(\":\n            isBlock = True\n            idx += 1\n\n        if toks[idx] == \"select\":\n            idx, sql = parse_sql(toks, idx, tables_with_alias, schema)\n            table_units.append((TABLE_TYPE[\"sql\"], sql))\n        else:\n            if idx &lt; len_ and toks[idx] == \"join\":\n                idx += 1  # skip join\n            idx, table_unit, table_name = parse_table_unit(\n                toks, idx, tables_with_alias, schema\n            )\n            table_units.append((TABLE_TYPE[\"table_unit\"], table_unit))\n            default_tables.append(table_name)\n        if idx &lt; len_ and toks[idx] == \"on\":\n            idx += 1  # skip on\n            idx, this_conds = parse_condition(\n                toks, idx, tables_with_alias, schema, default_tables\n            )\n            if len(conds) &gt; 0:\n                conds.append(\"and\")\n            conds.extend(this_conds)\n\n        if isBlock:\n            assert toks[idx] == \")\"\n            idx += 1\n        if idx &lt; len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n            break\n\n    return idx, table_units, conds, default_tables\n</code></pre>"},{"location":"reference/genlm/eval/domains/spider/spider_eval/schema/","title":"schema","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/schema/#genlm.eval.domains.spider.spider_eval.schema","title":"<code>genlm.eval.domains.spider.spider_eval.schema</code>","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/utils/","title":"utils","text":""},{"location":"reference/genlm/eval/domains/spider/spider_eval/utils/#genlm.eval.domains.spider.spider_eval.utils","title":"<code>genlm.eval.domains.spider.spider_eval.utils</code>","text":""}]}